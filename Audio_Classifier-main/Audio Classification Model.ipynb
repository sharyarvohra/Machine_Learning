{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boxed-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3554it [04:04, 16.27it/s]c:\\users\\tashikmoin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8323it [09:00, 24.67it/s]c:\\users\\tashikmoin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  warnings.warn(\n",
      "8327it [09:00, 27.92it/s]c:\\users\\tashikmoin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [09:23, 15.51it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "audio_file_path='tests/test_dog0.wav'\n",
    "librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(librosa_audio_data)\n",
    "wave_sample_rate, wave_audio = wav.read(audio_file_path)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(wave_audio)\n",
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)\n",
    "print(mfccs.shape)\n",
    "audio_dataset_path='dataset/audio/'\n",
    "metadata=pd.read_csv('dataset/metadata/dataset.csv')\n",
    "metadata.head()\n",
    "\n",
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    return mfccs_scaled_features\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acoustic-wagner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-215.79301, 71.66612, -131.81377, -52.091328,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-424.68677, 110.56227, -54.148235, 62.01074, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-459.56467, 122.800354, -47.92471, 53.265694,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-414.55377, 102.896904, -36.66495, 54.180405,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-447.397, 115.0954, -53.809113, 61.608585, 1....</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-215.79301, 71.66612, -131.81377, -52.091328,...          dog_bark\n",
       "1  [-424.68677, 110.56227, -54.148235, 62.01074, ...  children_playing\n",
       "2  [-459.56467, 122.800354, -47.92471, 53.265694,...  children_playing\n",
       "3  [-414.55377, 102.896904, -36.66495, 54.180405,...  children_playing\n",
       "4  [-447.397, 115.0954, -53.809113, 61.608585, 1....  children_playing"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "characteristic-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "wired-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "documentary-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-vessel",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "loose-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "indonesian-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "valuable-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "found-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(500,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "radical-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd33e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "140/140 [==============================] - 2s 9ms/step - loss: 7.4501 - accuracy: 0.1281 - val_loss: 2.2757 - val_accuracy: 0.1374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.27574, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 2/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 2.4099 - accuracy: 0.1426 - val_loss: 2.2318 - val_accuracy: 0.1505\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.27574 to 2.23181, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 3/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 2.2682 - accuracy: 0.1635 - val_loss: 2.1689 - val_accuracy: 0.1671\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.23181 to 2.16893, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 4/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 2.2041 - accuracy: 0.1787 - val_loss: 2.1045 - val_accuracy: 0.1992\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.16893 to 2.10447, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 5/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 2.1412 - accuracy: 0.2059 - val_loss: 1.9970 - val_accuracy: 0.2461\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.10447 to 1.99698, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 6/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 2.0901 - accuracy: 0.2123 - val_loss: 1.9533 - val_accuracy: 0.2742\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.99698 to 1.95330, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 7/1000\n",
      "140/140 [==============================] - ETA: 0s - loss: 2.0437 - accuracy: 0.23 - 1s 6ms/step - loss: 2.0466 - accuracy: 0.2336 - val_loss: 1.9156 - val_accuracy: 0.3234\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.95330 to 1.91563, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 8/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 2.0052 - accuracy: 0.2431 - val_loss: 1.8905 - val_accuracy: 0.3011\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.91563 to 1.89047, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 9/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.9767 - accuracy: 0.2640 - val_loss: 1.8115 - val_accuracy: 0.3423\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.89047 to 1.81151, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 10/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.9164 - accuracy: 0.2893 - val_loss: 1.7838 - val_accuracy: 0.3600\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.81151 to 1.78382, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 11/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.8541 - accuracy: 0.3162 - val_loss: 1.6712 - val_accuracy: 0.4041\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.78382 to 1.67120, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 12/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.7733 - accuracy: 0.3505 - val_loss: 1.5938 - val_accuracy: 0.4780\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.67120 to 1.59380, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 13/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 1.7107 - accuracy: 0.3777 - val_loss: 1.4796 - val_accuracy: 0.5077\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.59380 to 1.47960, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 14/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.6470 - accuracy: 0.4093 - val_loss: 1.4238 - val_accuracy: 0.5060\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.47960 to 1.42375, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 15/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.5931 - accuracy: 0.4332 - val_loss: 1.3687 - val_accuracy: 0.5392\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.42375 to 1.36875, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 16/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.5451 - accuracy: 0.4586 - val_loss: 1.3361 - val_accuracy: 0.5627\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.36875 to 1.33611, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 17/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.4960 - accuracy: 0.4829 - val_loss: 1.3126 - val_accuracy: 0.5529\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.33611 to 1.31263, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 18/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.4409 - accuracy: 0.4979 - val_loss: 1.2259 - val_accuracy: 0.6079\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.31263 to 1.22593, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 19/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.4088 - accuracy: 0.5157 - val_loss: 1.1800 - val_accuracy: 0.6291\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.22593 to 1.17996, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 20/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.3617 - accuracy: 0.5422 - val_loss: 1.1227 - val_accuracy: 0.6531\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.17996 to 1.12273, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 21/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.3108 - accuracy: 0.5548 - val_loss: 1.0905 - val_accuracy: 0.6457\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.12273 to 1.09054, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 22/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.2525 - accuracy: 0.5804 - val_loss: 1.0433 - val_accuracy: 0.6714\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.09054 to 1.04329, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 23/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.2296 - accuracy: 0.5860 - val_loss: 1.0029 - val_accuracy: 0.6829\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.04329 to 1.00288, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 24/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.1965 - accuracy: 0.6021 - val_loss: 0.9727 - val_accuracy: 0.6886\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.00288 to 0.97272, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 25/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.1530 - accuracy: 0.6123 - val_loss: 0.9745 - val_accuracy: 0.6726\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.97272\n",
      "Epoch 26/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.1206 - accuracy: 0.6198 - val_loss: 0.9166 - val_accuracy: 0.6961\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.97272 to 0.91663, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 27/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.0928 - accuracy: 0.6263 - val_loss: 0.9039 - val_accuracy: 0.6938\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.91663 to 0.90395, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 28/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.0590 - accuracy: 0.6438 - val_loss: 0.9043 - val_accuracy: 0.6938\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.90395\n",
      "Epoch 29/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.0345 - accuracy: 0.6540 - val_loss: 0.8514 - val_accuracy: 0.7149\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.90395 to 0.85140, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 30/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.0167 - accuracy: 0.6553 - val_loss: 0.8476 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.85140 to 0.84764, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 31/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 1.0093 - accuracy: 0.6660 - val_loss: 0.7933 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.84764 to 0.79335, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 32/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.9629 - accuracy: 0.6760 - val_loss: 0.8010 - val_accuracy: 0.7235\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.79335\n",
      "Epoch 33/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.9244 - accuracy: 0.6839 - val_loss: 0.7580 - val_accuracy: 0.7619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_loss improved from 0.79335 to 0.75797, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 34/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.9240 - accuracy: 0.6962 - val_loss: 0.7544 - val_accuracy: 0.7556\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.75797 to 0.75441, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 35/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.8972 - accuracy: 0.7004 - val_loss: 0.7361 - val_accuracy: 0.7687\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.75441 to 0.73608, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 36/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.8860 - accuracy: 0.7074 - val_loss: 0.7255 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.73608 to 0.72547, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 37/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.8680 - accuracy: 0.7074 - val_loss: 0.7030 - val_accuracy: 0.7865\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.72547 to 0.70296, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 38/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.8442 - accuracy: 0.7180 - val_loss: 0.7039 - val_accuracy: 0.7779\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.70296\n",
      "Epoch 39/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.8247 - accuracy: 0.7205 - val_loss: 0.6811 - val_accuracy: 0.7831\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.70296 to 0.68105, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 40/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.8414 - accuracy: 0.7210 - val_loss: 0.6891 - val_accuracy: 0.7762\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.68105\n",
      "Epoch 41/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.8091 - accuracy: 0.7290 - val_loss: 0.6764 - val_accuracy: 0.7916\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.68105 to 0.67637, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 42/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7861 - accuracy: 0.7337 - val_loss: 0.6414 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.67637 to 0.64138, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 43/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7559 - accuracy: 0.7500 - val_loss: 0.6434 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.64138\n",
      "Epoch 44/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7772 - accuracy: 0.7390 - val_loss: 0.6231 - val_accuracy: 0.8071\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.64138 to 0.62310, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 45/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7622 - accuracy: 0.7479 - val_loss: 0.6215 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.62310 to 0.62151, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 46/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.7195 - accuracy: 0.7556 - val_loss: 0.6123 - val_accuracy: 0.8071\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.62151 to 0.61231, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 47/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.7524 - accuracy: 0.7525 - val_loss: 0.6455 - val_accuracy: 0.7968\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.61231\n",
      "Epoch 48/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.7138 - accuracy: 0.7612 - val_loss: 0.5962 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.61231 to 0.59618, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 49/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7118 - accuracy: 0.7659 - val_loss: 0.5587 - val_accuracy: 0.8294\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.59618 to 0.55865, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 50/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7039 - accuracy: 0.7679 - val_loss: 0.5765 - val_accuracy: 0.8231\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.55865\n",
      "Epoch 51/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.7074 - accuracy: 0.7675 - val_loss: 0.5723 - val_accuracy: 0.8323\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.55865\n",
      "Epoch 52/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6833 - accuracy: 0.7780 - val_loss: 0.5664 - val_accuracy: 0.8254\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.55865\n",
      "Epoch 53/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.7844 - val_loss: 0.5383 - val_accuracy: 0.8432\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.55865 to 0.53831, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 54/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6706 - accuracy: 0.7774 - val_loss: 0.5446 - val_accuracy: 0.8311\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.53831\n",
      "Epoch 55/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6333 - accuracy: 0.7956 - val_loss: 0.5085 - val_accuracy: 0.8535\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.53831 to 0.50846, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 56/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.6553 - accuracy: 0.7844 - val_loss: 0.5267 - val_accuracy: 0.8540\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.50846\n",
      "Epoch 57/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6399 - accuracy: 0.7950 - val_loss: 0.5316 - val_accuracy: 0.8432\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.50846\n",
      "Epoch 58/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6387 - accuracy: 0.7903 - val_loss: 0.5117 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.50846\n",
      "Epoch 59/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.6325 - accuracy: 0.7907 - val_loss: 0.5284 - val_accuracy: 0.8420\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.50846\n",
      "Epoch 60/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6238 - accuracy: 0.7969 - val_loss: 0.5024 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.50846 to 0.50241, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 61/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6258 - accuracy: 0.7946 - val_loss: 0.5202 - val_accuracy: 0.8329\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.50241\n",
      "Epoch 62/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6165 - accuracy: 0.7969 - val_loss: 0.5065 - val_accuracy: 0.8460\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.50241\n",
      "Epoch 63/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6011 - accuracy: 0.7996 - val_loss: 0.5023 - val_accuracy: 0.8546\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.50241 to 0.50226, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 64/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5909 - accuracy: 0.8073 - val_loss: 0.4931 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.50226 to 0.49310, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 65/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5903 - accuracy: 0.8000 - val_loss: 0.5154 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.49310\n",
      "Epoch 66/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5816 - accuracy: 0.8031 - val_loss: 0.4806 - val_accuracy: 0.8552\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.49310 to 0.48061, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 67/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5825 - accuracy: 0.8092 - val_loss: 0.4984 - val_accuracy: 0.8517\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.48061\n",
      "Epoch 68/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5813 - accuracy: 0.8117 - val_loss: 0.4709 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.48061 to 0.47091, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 69/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5491 - accuracy: 0.8195 - val_loss: 0.4540 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.47091 to 0.45402, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 70/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5637 - accuracy: 0.8115 - val_loss: 0.4643 - val_accuracy: 0.8563\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.45402\n",
      "Epoch 71/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5500 - accuracy: 0.8195 - val_loss: 0.4724 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.45402\n",
      "Epoch 72/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5601 - accuracy: 0.8165 - val_loss: 0.4603 - val_accuracy: 0.8558\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.45402\n",
      "Epoch 73/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5607 - accuracy: 0.8176 - val_loss: 0.4744 - val_accuracy: 0.8512\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.45402\n",
      "Epoch 74/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5314 - accuracy: 0.8195 - val_loss: 0.4606 - val_accuracy: 0.8598\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.45402\n",
      "Epoch 75/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5561 - accuracy: 0.8198 - val_loss: 0.4524 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.45402 to 0.45244, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 76/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5419 - accuracy: 0.8219 - val_loss: 0.4502 - val_accuracy: 0.8638\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.45244 to 0.45018, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 77/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5536 - accuracy: 0.8159 - val_loss: 0.4626 - val_accuracy: 0.8586\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.45018\n",
      "Epoch 78/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.8205 - val_loss: 0.4600 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.45018\n",
      "Epoch 79/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5481 - accuracy: 0.8220 - val_loss: 0.4563 - val_accuracy: 0.8580\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.45018\n",
      "Epoch 80/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.5485 - accuracy: 0.8238 - val_loss: 0.4549 - val_accuracy: 0.8638\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.45018\n",
      "Epoch 81/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.5281 - accuracy: 0.8213 - val_loss: 0.4465 - val_accuracy: 0.8535\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.45018 to 0.44649, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 82/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.5254 - accuracy: 0.8216 - val_loss: 0.4383 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.44649 to 0.43829, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 83/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.5155 - accuracy: 0.8316 - val_loss: 0.4286 - val_accuracy: 0.8615\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.43829 to 0.42857, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 84/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.5042 - accuracy: 0.8342 - val_loss: 0.4345 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.42857\n",
      "Epoch 85/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.5206 - accuracy: 0.8265 - val_loss: 0.4272 - val_accuracy: 0.8689\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.42857 to 0.42718, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 86/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5192 - accuracy: 0.8282 - val_loss: 0.4236 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.42718 to 0.42362, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 87/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.4868 - accuracy: 0.8391 - val_loss: 0.4144 - val_accuracy: 0.8712\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.42362 to 0.41438, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 88/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5184 - accuracy: 0.8298 - val_loss: 0.4423 - val_accuracy: 0.8620\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.41438\n",
      "Epoch 89/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5201 - accuracy: 0.8349 - val_loss: 0.3973 - val_accuracy: 0.8746\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.41438 to 0.39729, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 90/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5029 - accuracy: 0.8408 - val_loss: 0.4188 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.39729\n",
      "Epoch 91/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.5080 - accuracy: 0.8384 - val_loss: 0.4127 - val_accuracy: 0.8718\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.39729\n",
      "Epoch 92/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5106 - accuracy: 0.8341 - val_loss: 0.4234 - val_accuracy: 0.8689\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.39729\n",
      "Epoch 93/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4939 - accuracy: 0.8408 - val_loss: 0.3954 - val_accuracy: 0.8724\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.39729 to 0.39543, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 94/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5006 - accuracy: 0.8369 - val_loss: 0.4056 - val_accuracy: 0.8781\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.39543\n",
      "Epoch 95/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4666 - accuracy: 0.8428 - val_loss: 0.4125 - val_accuracy: 0.8712\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.39543\n",
      "Epoch 96/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4652 - accuracy: 0.8435 - val_loss: 0.4147 - val_accuracy: 0.8792\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.39543\n",
      "Epoch 97/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4755 - accuracy: 0.8409 - val_loss: 0.3957 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.39543\n",
      "Epoch 98/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4480 - accuracy: 0.8478 - val_loss: 0.3909 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.39543 to 0.39092, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 99/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4671 - accuracy: 0.8503 - val_loss: 0.4006 - val_accuracy: 0.8741\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.39092\n",
      "Epoch 100/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4634 - accuracy: 0.8511 - val_loss: 0.3986 - val_accuracy: 0.8758\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.39092\n",
      "Epoch 101/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4790 - accuracy: 0.8419 - val_loss: 0.3857 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.39092 to 0.38574, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 102/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4836 - accuracy: 0.8464 - val_loss: 0.4042 - val_accuracy: 0.8689\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.38574\n",
      "Epoch 103/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4578 - accuracy: 0.8531 - val_loss: 0.3943 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.38574\n",
      "Epoch 104/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4842 - accuracy: 0.8404 - val_loss: 0.3868 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.38574\n",
      "Epoch 105/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4710 - accuracy: 0.8465 - val_loss: 0.3890 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.38574\n",
      "Epoch 106/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4421 - accuracy: 0.8555 - val_loss: 0.3804 - val_accuracy: 0.8792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss improved from 0.38574 to 0.38041, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 107/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4324 - accuracy: 0.8551 - val_loss: 0.3781 - val_accuracy: 0.8895\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.38041 to 0.37809, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 108/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4459 - accuracy: 0.8567 - val_loss: 0.3906 - val_accuracy: 0.8746\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.37809\n",
      "Epoch 109/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4665 - accuracy: 0.8533 - val_loss: 0.3950 - val_accuracy: 0.8741\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.37809\n",
      "Epoch 110/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4397 - accuracy: 0.8590 - val_loss: 0.3789 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.37809\n",
      "Epoch 111/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4619 - accuracy: 0.8537 - val_loss: 0.3807 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.37809\n",
      "Epoch 112/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4406 - accuracy: 0.8557 - val_loss: 0.3887 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.37809\n",
      "Epoch 113/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4607 - accuracy: 0.8535 - val_loss: 0.3960 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.37809\n",
      "Epoch 114/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4576 - accuracy: 0.8535 - val_loss: 0.4078 - val_accuracy: 0.8729\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.37809\n",
      "Epoch 115/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4369 - accuracy: 0.8558 - val_loss: 0.3812 - val_accuracy: 0.8792\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.37809\n",
      "Epoch 116/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4385 - accuracy: 0.8565 - val_loss: 0.3811 - val_accuracy: 0.8775\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.37809\n",
      "Epoch 117/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4486 - accuracy: 0.8553 - val_loss: 0.3750 - val_accuracy: 0.8792\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.37809 to 0.37501, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 118/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4226 - accuracy: 0.8590 - val_loss: 0.3833 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.37501\n",
      "Epoch 119/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4351 - accuracy: 0.8588 - val_loss: 0.3993 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.37501\n",
      "Epoch 120/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4275 - accuracy: 0.8623 - val_loss: 0.3882 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.37501\n",
      "Epoch 121/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4200 - accuracy: 0.8659 - val_loss: 0.3884 - val_accuracy: 0.8792\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.37501\n",
      "Epoch 122/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4246 - accuracy: 0.8613 - val_loss: 0.4013 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.37501\n",
      "Epoch 123/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4450 - accuracy: 0.8616 - val_loss: 0.3875 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.37501\n",
      "Epoch 124/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4260 - accuracy: 0.8631 - val_loss: 0.3908 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.37501\n",
      "Epoch 125/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4400 - accuracy: 0.8600 - val_loss: 0.3850 - val_accuracy: 0.8758\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.37501\n",
      "Epoch 126/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4158 - accuracy: 0.8647 - val_loss: 0.3895 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.37501\n",
      "Epoch 127/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4334 - accuracy: 0.8593 - val_loss: 0.3831 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.37501\n",
      "Epoch 128/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4313 - accuracy: 0.8656 - val_loss: 0.3917 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.37501\n",
      "Epoch 129/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4253 - accuracy: 0.8651 - val_loss: 0.3817 - val_accuracy: 0.8821\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.37501\n",
      "Epoch 130/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4224 - accuracy: 0.8600 - val_loss: 0.3779 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.37501\n",
      "Epoch 131/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4267 - accuracy: 0.8614 - val_loss: 0.3700 - val_accuracy: 0.8872\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.37501 to 0.37002, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 132/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4069 - accuracy: 0.8719 - val_loss: 0.3812 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.37002\n",
      "Epoch 133/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4337 - accuracy: 0.8597 - val_loss: 0.3619 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.37002 to 0.36191, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 134/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4167 - accuracy: 0.8621 - val_loss: 0.3728 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.36191\n",
      "Epoch 135/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4144 - accuracy: 0.8693 - val_loss: 0.3744 - val_accuracy: 0.8895\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.36191\n",
      "Epoch 136/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.4190 - accuracy: 0.8639 - val_loss: 0.3942 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.36191\n",
      "Epoch 137/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3898 - accuracy: 0.8732 - val_loss: 0.3703 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.36191\n",
      "Epoch 138/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4325 - accuracy: 0.8611 - val_loss: 0.3713 - val_accuracy: 0.8872\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.36191\n",
      "Epoch 139/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4177 - accuracy: 0.8653 - val_loss: 0.3811 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.36191\n",
      "Epoch 140/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4033 - accuracy: 0.8722 - val_loss: 0.3632 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.36191\n",
      "Epoch 141/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3906 - accuracy: 0.8760 - val_loss: 0.3759 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.36191\n",
      "Epoch 142/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3944 - accuracy: 0.8716 - val_loss: 0.3550 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.36191 to 0.35497, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 143/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4023 - accuracy: 0.8689 - val_loss: 0.3951 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.35497\n",
      "Epoch 144/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4004 - accuracy: 0.8663 - val_loss: 0.3677 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.35497\n",
      "Epoch 145/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3753 - accuracy: 0.8762 - val_loss: 0.3703 - val_accuracy: 0.8809\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.35497\n",
      "Epoch 146/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4060 - accuracy: 0.8680 - val_loss: 0.3545 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.35497 to 0.35455, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 147/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4098 - accuracy: 0.8664 - val_loss: 0.3588 - val_accuracy: 0.8849\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.35455\n",
      "Epoch 148/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4123 - accuracy: 0.8646 - val_loss: 0.3610 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.35455\n",
      "Epoch 149/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4106 - accuracy: 0.8666 - val_loss: 0.3745 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.35455\n",
      "Epoch 150/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3937 - accuracy: 0.8749 - val_loss: 0.3749 - val_accuracy: 0.8872\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.35455\n",
      "Epoch 151/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3729 - accuracy: 0.8786 - val_loss: 0.3866 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.35455\n",
      "Epoch 152/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3682 - accuracy: 0.8816 - val_loss: 0.3703 - val_accuracy: 0.8872\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.35455\n",
      "Epoch 153/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4019 - accuracy: 0.8730 - val_loss: 0.3768 - val_accuracy: 0.8855\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.35455\n",
      "Epoch 154/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3788 - accuracy: 0.8783 - val_loss: 0.3895 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.35455\n",
      "Epoch 155/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3848 - accuracy: 0.8720 - val_loss: 0.3585 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.35455\n",
      "Epoch 156/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3995 - accuracy: 0.8747 - val_loss: 0.3561 - val_accuracy: 0.8884\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.35455\n",
      "Epoch 157/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3680 - accuracy: 0.8770 - val_loss: 0.3763 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.35455\n",
      "Epoch 158/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3599 - accuracy: 0.8776 - val_loss: 0.3722 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.35455\n",
      "Epoch 159/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3985 - accuracy: 0.8742 - val_loss: 0.3785 - val_accuracy: 0.8901\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.35455\n",
      "Epoch 160/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3880 - accuracy: 0.8777 - val_loss: 0.3724 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.35455\n",
      "Epoch 161/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3917 - accuracy: 0.8760 - val_loss: 0.3902 - val_accuracy: 0.8769\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.35455\n",
      "Epoch 162/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3674 - accuracy: 0.8805 - val_loss: 0.3685 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.35455\n",
      "Epoch 163/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3886 - accuracy: 0.8710 - val_loss: 0.3784 - val_accuracy: 0.8895\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.35455\n",
      "Epoch 164/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4001 - accuracy: 0.8700 - val_loss: 0.3744 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.35455\n",
      "Epoch 165/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3706 - accuracy: 0.8813 - val_loss: 0.3595 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.35455\n",
      "Epoch 166/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3977 - accuracy: 0.8747 - val_loss: 0.3868 - val_accuracy: 0.8809\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.35455\n",
      "Epoch 167/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3838 - accuracy: 0.8759 - val_loss: 0.3362 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.35455 to 0.33623, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 168/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3753 - accuracy: 0.8800 - val_loss: 0.3453 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.33623\n",
      "Epoch 169/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3883 - accuracy: 0.8813 - val_loss: 0.3459 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.33623\n",
      "Epoch 170/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4043 - accuracy: 0.8743 - val_loss: 0.3714 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.33623\n",
      "Epoch 171/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3621 - accuracy: 0.8783 - val_loss: 0.3510 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.33623\n",
      "Epoch 172/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3647 - accuracy: 0.8843 - val_loss: 0.3533 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.33623\n",
      "Epoch 173/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3875 - accuracy: 0.8759 - val_loss: 0.3751 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.33623\n",
      "Epoch 174/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3735 - accuracy: 0.8806 - val_loss: 0.3640 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.33623\n",
      "Epoch 175/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3683 - accuracy: 0.8836 - val_loss: 0.3530 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.33623\n",
      "Epoch 176/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3856 - accuracy: 0.8766 - val_loss: 0.3356 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.33623 to 0.33563, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 177/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3525 - accuracy: 0.8835 - val_loss: 0.3356 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.33563\n",
      "Epoch 178/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3634 - accuracy: 0.8795 - val_loss: 0.3381 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.33563\n",
      "Epoch 179/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3563 - accuracy: 0.8859 - val_loss: 0.3479 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.33563\n",
      "Epoch 180/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3578 - accuracy: 0.8862 - val_loss: 0.3554 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.33563\n",
      "Epoch 181/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3739 - accuracy: 0.8806 - val_loss: 0.3564 - val_accuracy: 0.8890\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.33563\n",
      "Epoch 182/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3801 - accuracy: 0.8793 - val_loss: 0.3381 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.33563\n",
      "Epoch 183/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3586 - accuracy: 0.8856 - val_loss: 0.3328 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.33563 to 0.33280, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 184/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3641 - accuracy: 0.8836 - val_loss: 0.3560 - val_accuracy: 0.8901\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.33280\n",
      "Epoch 185/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3639 - accuracy: 0.8803 - val_loss: 0.3540 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.33280\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3435 - accuracy: 0.8868 - val_loss: 0.3335 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.33280\n",
      "Epoch 187/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3631 - accuracy: 0.8852 - val_loss: 0.3291 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.33280 to 0.32907, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 188/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3759 - accuracy: 0.8797 - val_loss: 0.3293 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.32907\n",
      "Epoch 189/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3532 - accuracy: 0.8860 - val_loss: 0.3502 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.32907\n",
      "Epoch 190/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3386 - accuracy: 0.8936 - val_loss: 0.3464 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.32907\n",
      "Epoch 191/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3555 - accuracy: 0.8826 - val_loss: 0.3436 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.32907\n",
      "Epoch 192/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3580 - accuracy: 0.8856 - val_loss: 0.3552 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.32907\n",
      "Epoch 193/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3481 - accuracy: 0.8886 - val_loss: 0.3428 - val_accuracy: 0.8901\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.32907\n",
      "Epoch 194/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3550 - accuracy: 0.8906 - val_loss: 0.3314 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.32907\n",
      "Epoch 195/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3566 - accuracy: 0.8839 - val_loss: 0.3398 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.32907\n",
      "Epoch 196/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3716 - accuracy: 0.8800 - val_loss: 0.3557 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.32907\n",
      "Epoch 197/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3593 - accuracy: 0.8846 - val_loss: 0.3360 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.32907\n",
      "Epoch 198/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3535 - accuracy: 0.8916 - val_loss: 0.3232 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.32907 to 0.32316, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 199/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3679 - accuracy: 0.8880 - val_loss: 0.3463 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.32316\n",
      "Epoch 200/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3533 - accuracy: 0.8918 - val_loss: 0.3582 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.32316\n",
      "Epoch 201/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3704 - accuracy: 0.8829 - val_loss: 0.3531 - val_accuracy: 0.8935\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.32316\n",
      "Epoch 202/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3494 - accuracy: 0.8885 - val_loss: 0.3343 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.32316\n",
      "Epoch 203/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3333 - accuracy: 0.8936 - val_loss: 0.3416 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.32316\n",
      "Epoch 204/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3537 - accuracy: 0.8898 - val_loss: 0.3526 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.32316\n",
      "Epoch 205/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3512 - accuracy: 0.8879 - val_loss: 0.3400 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.32316\n",
      "Epoch 206/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3537 - accuracy: 0.8875 - val_loss: 0.3312 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.32316\n",
      "Epoch 207/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3647 - accuracy: 0.8845 - val_loss: 0.3454 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.32316\n",
      "Epoch 208/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3524 - accuracy: 0.8839 - val_loss: 0.3470 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.32316\n",
      "Epoch 209/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3394 - accuracy: 0.8946 - val_loss: 0.3387 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.32316\n",
      "Epoch 210/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3365 - accuracy: 0.8945 - val_loss: 0.3502 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.32316\n",
      "Epoch 211/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3510 - accuracy: 0.8886 - val_loss: 0.3520 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.32316\n",
      "Epoch 212/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3360 - accuracy: 0.8913 - val_loss: 0.3316 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.32316\n",
      "Epoch 213/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8953 - val_loss: 0.3514 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.32316\n",
      "Epoch 214/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3276 - accuracy: 0.8968 - val_loss: 0.3361 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.32316\n",
      "Epoch 215/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3208 - accuracy: 0.8998 - val_loss: 0.3444 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.32316\n",
      "Epoch 216/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3359 - accuracy: 0.8921 - val_loss: 0.3596 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.32316\n",
      "Epoch 217/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3256 - accuracy: 0.8952 - val_loss: 0.3587 - val_accuracy: 0.8964\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.32316\n",
      "Epoch 218/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3567 - accuracy: 0.8915 - val_loss: 0.3581 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.32316\n",
      "Epoch 219/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3335 - accuracy: 0.8939 - val_loss: 0.3389 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.32316\n",
      "Epoch 220/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3296 - accuracy: 0.8961 - val_loss: 0.3727 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.32316\n",
      "Epoch 221/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3548 - accuracy: 0.8888 - val_loss: 0.3550 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.32316\n",
      "Epoch 222/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3414 - accuracy: 0.8918 - val_loss: 0.3475 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.32316\n",
      "Epoch 223/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3702 - accuracy: 0.8865 - val_loss: 0.3184 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.32316 to 0.31839, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 224/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3294 - accuracy: 0.8942 - val_loss: 0.3459 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.31839\n",
      "Epoch 225/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3335 - accuracy: 0.8981 - val_loss: 0.3439 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.31839\n",
      "Epoch 226/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3316 - accuracy: 0.8933 - val_loss: 0.3565 - val_accuracy: 0.8855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00226: val_loss did not improve from 0.31839\n",
      "Epoch 227/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3342 - accuracy: 0.8976 - val_loss: 0.3568 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.31839\n",
      "Epoch 228/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3312 - accuracy: 0.8959 - val_loss: 0.3327 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.31839\n",
      "Epoch 229/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3188 - accuracy: 0.9014 - val_loss: 0.3365 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.31839\n",
      "Epoch 230/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3055 - accuracy: 0.8995 - val_loss: 0.3508 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.31839\n",
      "Epoch 231/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3470 - accuracy: 0.8946 - val_loss: 0.3447 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.31839\n",
      "Epoch 232/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3108 - accuracy: 0.8995 - val_loss: 0.3500 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.31839\n",
      "Epoch 233/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3450 - accuracy: 0.8976 - val_loss: 0.3489 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.31839\n",
      "Epoch 234/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3278 - accuracy: 0.8979 - val_loss: 0.3436 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.31839\n",
      "Epoch 235/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3358 - accuracy: 0.8958 - val_loss: 0.3478 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.31839\n",
      "Epoch 236/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3156 - accuracy: 0.9015 - val_loss: 0.3574 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.31839\n",
      "Epoch 237/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3339 - accuracy: 0.8965 - val_loss: 0.3263 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.31839\n",
      "Epoch 238/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3234 - accuracy: 0.9001 - val_loss: 0.3409 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.31839\n",
      "Epoch 239/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3375 - accuracy: 0.8985 - val_loss: 0.3358 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.31839\n",
      "Epoch 240/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3394 - accuracy: 0.8948 - val_loss: 0.3332 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.31839\n",
      "Epoch 241/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3188 - accuracy: 0.9024 - val_loss: 0.3420 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.31839\n",
      "Epoch 242/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3171 - accuracy: 0.8998 - val_loss: 0.3326 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.31839\n",
      "Epoch 243/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3337 - accuracy: 0.8948 - val_loss: 0.3445 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.31839\n",
      "Epoch 244/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3151 - accuracy: 0.8989 - val_loss: 0.3439 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.31839\n",
      "Epoch 245/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3190 - accuracy: 0.8965 - val_loss: 0.3418 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.31839\n",
      "Epoch 246/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3257 - accuracy: 0.8999 - val_loss: 0.3567 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.31839\n",
      "Epoch 247/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3358 - accuracy: 0.8948 - val_loss: 0.3345 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.31839\n",
      "Epoch 248/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3098 - accuracy: 0.9024 - val_loss: 0.3297 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.31839\n",
      "Epoch 249/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3290 - accuracy: 0.8991 - val_loss: 0.3261 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.31839\n",
      "Epoch 250/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3137 - accuracy: 0.9016 - val_loss: 0.3260 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.31839\n",
      "Epoch 251/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8996 - val_loss: 0.3502 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.31839\n",
      "Epoch 252/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3064 - accuracy: 0.8982 - val_loss: 0.3527 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.31839\n",
      "Epoch 253/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3387 - accuracy: 0.8989 - val_loss: 0.3504 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.31839\n",
      "Epoch 254/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3183 - accuracy: 0.8976 - val_loss: 0.3508 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.31839\n",
      "Epoch 255/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3107 - accuracy: 0.9001 - val_loss: 0.3316 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.31839\n",
      "Epoch 256/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3072 - accuracy: 0.9014 - val_loss: 0.3326 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.31839\n",
      "Epoch 257/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3110 - accuracy: 0.8992 - val_loss: 0.3284 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.31839\n",
      "Epoch 258/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3174 - accuracy: 0.9042 - val_loss: 0.3343 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.31839\n",
      "Epoch 259/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3138 - accuracy: 0.9029 - val_loss: 0.3270 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.31839\n",
      "Epoch 260/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3290 - accuracy: 0.8958 - val_loss: 0.3261 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.31839\n",
      "Epoch 261/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3145 - accuracy: 0.8982 - val_loss: 0.3387 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.31839\n",
      "Epoch 262/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3107 - accuracy: 0.9061 - val_loss: 0.3280 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.31839\n",
      "Epoch 263/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3170 - accuracy: 0.8995 - val_loss: 0.3229 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.31839\n",
      "Epoch 264/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3094 - accuracy: 0.8988 - val_loss: 0.3243 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.31839\n",
      "Epoch 265/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3178 - accuracy: 0.9004 - val_loss: 0.3166 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.31839 to 0.31663, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 266/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2999 - accuracy: 0.9044 - val_loss: 0.3290 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.31663\n",
      "Epoch 267/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3243 - accuracy: 0.8995 - val_loss: 0.3227 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.31663\n",
      "Epoch 268/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3175 - accuracy: 0.8991 - val_loss: 0.3217 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.31663\n",
      "Epoch 269/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3192 - accuracy: 0.9019 - val_loss: 0.3193 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.31663\n",
      "Epoch 270/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3080 - accuracy: 0.9002 - val_loss: 0.3464 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.31663\n",
      "Epoch 271/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3063 - accuracy: 0.9037 - val_loss: 0.3305 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.31663\n",
      "Epoch 272/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3256 - accuracy: 0.8975 - val_loss: 0.3452 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.31663\n",
      "Epoch 273/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2973 - accuracy: 0.9052 - val_loss: 0.3240 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.31663\n",
      "Epoch 274/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3275 - accuracy: 0.8985 - val_loss: 0.3280 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.31663\n",
      "Epoch 275/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3243 - accuracy: 0.8978 - val_loss: 0.3210 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.31663\n",
      "Epoch 276/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3177 - accuracy: 0.8999 - val_loss: 0.3213 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.31663\n",
      "Epoch 277/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2962 - accuracy: 0.9092 - val_loss: 0.3371 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.31663\n",
      "Epoch 278/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.9011 - val_loss: 0.3361 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.31663\n",
      "Epoch 279/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.9011 - val_loss: 0.3312 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.31663\n",
      "Epoch 280/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3331 - accuracy: 0.8948 - val_loss: 0.3344 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.31663\n",
      "Epoch 281/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3137 - accuracy: 0.9016 - val_loss: 0.3434 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.31663\n",
      "Epoch 282/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2876 - accuracy: 0.9071 - val_loss: 0.3467 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.31663\n",
      "Epoch 283/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3136 - accuracy: 0.9067 - val_loss: 0.3336 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.31663\n",
      "Epoch 284/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3146 - accuracy: 0.9047 - val_loss: 0.3454 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.31663\n",
      "Epoch 285/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2986 - accuracy: 0.9037 - val_loss: 0.3367 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.31663\n",
      "Epoch 286/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3073 - accuracy: 0.9048 - val_loss: 0.3289 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.31663\n",
      "Epoch 287/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3135 - accuracy: 0.9021 - val_loss: 0.3186 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.31663\n",
      "Epoch 288/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2897 - accuracy: 0.9055 - val_loss: 0.3421 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.31663\n",
      "Epoch 289/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3172 - accuracy: 0.9031 - val_loss: 0.3334 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.31663\n",
      "Epoch 290/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.9001 - val_loss: 0.3367 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.31663\n",
      "Epoch 291/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3222 - accuracy: 0.9037 - val_loss: 0.3298 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.31663\n",
      "Epoch 292/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3123 - accuracy: 0.9022 - val_loss: 0.3376 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.31663\n",
      "Epoch 293/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.9029 - val_loss: 0.3235 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.31663\n",
      "Epoch 294/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3036 - accuracy: 0.9081 - val_loss: 0.3100 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.31663 to 0.31000, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 295/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3088 - accuracy: 0.9055 - val_loss: 0.3019 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.31000 to 0.30185, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 296/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2940 - accuracy: 0.9028 - val_loss: 0.3252 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.30185\n",
      "Epoch 297/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3101 - accuracy: 0.9054 - val_loss: 0.3239 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.30185\n",
      "Epoch 298/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3081 - accuracy: 0.9014 - val_loss: 0.3406 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.30185\n",
      "Epoch 299/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2831 - accuracy: 0.9110 - val_loss: 0.3361 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.30185\n",
      "Epoch 300/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2929 - accuracy: 0.9069 - val_loss: 0.3387 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.30185\n",
      "Epoch 301/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2939 - accuracy: 0.9092 - val_loss: 0.3296 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.30185\n",
      "Epoch 302/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3041 - accuracy: 0.9051 - val_loss: 0.3320 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.30185\n",
      "Epoch 303/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3067 - accuracy: 0.9045 - val_loss: 0.3372 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.30185\n",
      "Epoch 304/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3051 - accuracy: 0.9061 - val_loss: 0.3329 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.30185\n",
      "Epoch 305/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2979 - accuracy: 0.9052 - val_loss: 0.3432 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.30185\n",
      "Epoch 306/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.9101 - val_loss: 0.3375 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.30185\n",
      "Epoch 307/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.9069 - val_loss: 0.3582 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.30185\n",
      "Epoch 308/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.3132 - accuracy: 0.9016 - val_loss: 0.3501 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.30185\n",
      "Epoch 309/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2875 - accuracy: 0.9111 - val_loss: 0.3303 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.30185\n",
      "Epoch 310/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3073 - accuracy: 0.9015 - val_loss: 0.3279 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.30185\n",
      "Epoch 311/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3067 - accuracy: 0.9032 - val_loss: 0.3329 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.30185\n",
      "Epoch 312/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2746 - accuracy: 0.9174 - val_loss: 0.3066 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.30185\n",
      "Epoch 313/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2856 - accuracy: 0.9111 - val_loss: 0.3334 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.30185\n",
      "Epoch 314/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2928 - accuracy: 0.9044 - val_loss: 0.3073 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.30185\n",
      "Epoch 315/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2973 - accuracy: 0.9039 - val_loss: 0.3080 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.30185\n",
      "Epoch 316/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2911 - accuracy: 0.9071 - val_loss: 0.3330 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.30185\n",
      "Epoch 317/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3051 - accuracy: 0.9031 - val_loss: 0.3310 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.30185\n",
      "Epoch 318/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2900 - accuracy: 0.9081 - val_loss: 0.3297 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.30185\n",
      "Epoch 319/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2995 - accuracy: 0.9019 - val_loss: 0.3322 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.30185\n",
      "Epoch 320/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2841 - accuracy: 0.9120 - val_loss: 0.3405 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.30185\n",
      "Epoch 321/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2924 - accuracy: 0.9065 - val_loss: 0.3337 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.30185\n",
      "Epoch 322/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2805 - accuracy: 0.9125 - val_loss: 0.3577 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.30185\n",
      "Epoch 323/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3031 - accuracy: 0.9108 - val_loss: 0.3344 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.30185\n",
      "Epoch 324/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3029 - accuracy: 0.9068 - val_loss: 0.3338 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.30185\n",
      "Epoch 325/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3004 - accuracy: 0.9067 - val_loss: 0.3228 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.30185\n",
      "Epoch 326/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.9054 - val_loss: 0.3308 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.30185\n",
      "Epoch 327/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2972 - accuracy: 0.9127 - val_loss: 0.3430 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.30185\n",
      "Epoch 328/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2708 - accuracy: 0.9168 - val_loss: 0.3344 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.30185\n",
      "Epoch 329/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2974 - accuracy: 0.9092 - val_loss: 0.3230 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.30185\n",
      "Epoch 330/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3110 - accuracy: 0.9039 - val_loss: 0.3523 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.30185\n",
      "Epoch 331/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3025 - accuracy: 0.9098 - val_loss: 0.3353 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.30185\n",
      "Epoch 332/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3108 - accuracy: 0.9077 - val_loss: 0.3676 - val_accuracy: 0.8987\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.30185\n",
      "Epoch 333/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3001 - accuracy: 0.9061 - val_loss: 0.3290 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.30185\n",
      "Epoch 334/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3055 - accuracy: 0.9049 - val_loss: 0.3295 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.30185\n",
      "Epoch 335/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2702 - accuracy: 0.9158 - val_loss: 0.3439 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.30185\n",
      "Epoch 336/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2910 - accuracy: 0.9101 - val_loss: 0.3338 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.30185\n",
      "Epoch 337/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2870 - accuracy: 0.9124 - val_loss: 0.3378 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.30185\n",
      "Epoch 338/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2871 - accuracy: 0.9127 - val_loss: 0.3512 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.30185\n",
      "Epoch 339/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2817 - accuracy: 0.9102 - val_loss: 0.3331 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.30185\n",
      "Epoch 340/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3029 - accuracy: 0.9130 - val_loss: 0.3336 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.30185\n",
      "Epoch 341/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2786 - accuracy: 0.9110 - val_loss: 0.3524 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.30185\n",
      "Epoch 342/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2679 - accuracy: 0.9138 - val_loss: 0.3304 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.30185\n",
      "Epoch 343/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2714 - accuracy: 0.9105 - val_loss: 0.3315 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.30185\n",
      "Epoch 344/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2967 - accuracy: 0.9081 - val_loss: 0.3356 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.30185\n",
      "Epoch 345/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3122 - accuracy: 0.9101 - val_loss: 0.3280 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.30185\n",
      "Epoch 346/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2977 - accuracy: 0.9098 - val_loss: 0.3379 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.30185\n",
      "Epoch 347/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2765 - accuracy: 0.9108 - val_loss: 0.3383 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.30185\n",
      "Epoch 348/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2890 - accuracy: 0.9107 - val_loss: 0.3524 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.30185\n",
      "Epoch 349/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2730 - accuracy: 0.9124 - val_loss: 0.3492 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.30185\n",
      "Epoch 350/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2956 - accuracy: 0.9085 - val_loss: 0.3495 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.30185\n",
      "Epoch 351/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2900 - accuracy: 0.9068 - val_loss: 0.3221 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.30185\n",
      "Epoch 352/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2989 - accuracy: 0.9088 - val_loss: 0.3467 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.30185\n",
      "Epoch 353/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2674 - accuracy: 0.9121 - val_loss: 0.3274 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.30185\n",
      "Epoch 354/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2894 - accuracy: 0.9067 - val_loss: 0.3116 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.30185\n",
      "Epoch 355/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2662 - accuracy: 0.9144 - val_loss: 0.3267 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.30185\n",
      "Epoch 356/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2923 - accuracy: 0.9108 - val_loss: 0.3228 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.30185\n",
      "Epoch 357/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2971 - accuracy: 0.9095 - val_loss: 0.3417 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.30185\n",
      "Epoch 358/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2741 - accuracy: 0.9144 - val_loss: 0.3337 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.30185\n",
      "Epoch 359/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.9150 - val_loss: 0.3222 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.30185\n",
      "Epoch 360/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2758 - accuracy: 0.9134 - val_loss: 0.3293 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.30185\n",
      "Epoch 361/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2791 - accuracy: 0.9170 - val_loss: 0.3069 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.30185\n",
      "Epoch 362/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2826 - accuracy: 0.9121 - val_loss: 0.3348 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.30185\n",
      "Epoch 363/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2918 - accuracy: 0.9089 - val_loss: 0.3367 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.30185\n",
      "Epoch 364/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3025 - accuracy: 0.9124 - val_loss: 0.3227 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.30185\n",
      "Epoch 365/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2877 - accuracy: 0.9104 - val_loss: 0.3320 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.30185\n",
      "Epoch 366/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2669 - accuracy: 0.9188 - val_loss: 0.3526 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.30185\n",
      "Epoch 367/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2775 - accuracy: 0.9102 - val_loss: 0.3266 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.30185\n",
      "Epoch 368/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3140 - accuracy: 0.9045 - val_loss: 0.3154 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.30185\n",
      "Epoch 369/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2943 - accuracy: 0.9111 - val_loss: 0.2965 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.30185 to 0.29653, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 370/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2562 - accuracy: 0.9164 - val_loss: 0.3074 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.29653\n",
      "Epoch 371/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2814 - accuracy: 0.9125 - val_loss: 0.3096 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.29653\n",
      "Epoch 372/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2702 - accuracy: 0.9173 - val_loss: 0.3106 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.29653\n",
      "Epoch 373/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2851 - accuracy: 0.9077 - val_loss: 0.3184 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.29653\n",
      "Epoch 374/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2847 - accuracy: 0.9084 - val_loss: 0.3178 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.29653\n",
      "Epoch 375/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2992 - accuracy: 0.9075 - val_loss: 0.3403 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.29653\n",
      "Epoch 376/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2914 - accuracy: 0.9078 - val_loss: 0.3143 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.29653\n",
      "Epoch 377/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2958 - accuracy: 0.9071 - val_loss: 0.3192 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.29653\n",
      "Epoch 378/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2808 - accuracy: 0.9152 - val_loss: 0.3170 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.29653\n",
      "Epoch 379/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2977 - accuracy: 0.9102 - val_loss: 0.3561 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.29653\n",
      "Epoch 380/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2888 - accuracy: 0.9115 - val_loss: 0.3505 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.29653\n",
      "Epoch 381/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2945 - accuracy: 0.9089 - val_loss: 0.3383 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.29653\n",
      "Epoch 382/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2741 - accuracy: 0.9171 - val_loss: 0.3335 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.29653\n",
      "Epoch 383/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2753 - accuracy: 0.9165 - val_loss: 0.3279 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.29653\n",
      "Epoch 384/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2860 - accuracy: 0.9120 - val_loss: 0.3201 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.29653\n",
      "Epoch 385/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2757 - accuracy: 0.9154 - val_loss: 0.3422 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.29653\n",
      "Epoch 386/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2865 - accuracy: 0.9178 - val_loss: 0.3460 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.29653\n",
      "Epoch 387/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2680 - accuracy: 0.9173 - val_loss: 0.3365 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.29653\n",
      "Epoch 388/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2826 - accuracy: 0.9120 - val_loss: 0.3292 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.29653\n",
      "Epoch 389/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2991 - accuracy: 0.9084 - val_loss: 0.3252 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.29653\n",
      "Epoch 390/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2804 - accuracy: 0.9105 - val_loss: 0.3444 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.29653\n",
      "Epoch 391/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2822 - accuracy: 0.9144 - val_loss: 0.3355 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.29653\n",
      "Epoch 392/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2642 - accuracy: 0.9181 - val_loss: 0.3399 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.29653\n",
      "Epoch 393/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2825 - accuracy: 0.9168 - val_loss: 0.3421 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.29653\n",
      "Epoch 394/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2801 - accuracy: 0.9130 - val_loss: 0.3359 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.29653\n",
      "Epoch 395/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2720 - accuracy: 0.9150 - val_loss: 0.3178 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.29653\n",
      "Epoch 396/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2706 - accuracy: 0.9183 - val_loss: 0.3362 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.29653\n",
      "Epoch 397/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2818 - accuracy: 0.9135 - val_loss: 0.3425 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.29653\n",
      "Epoch 398/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2778 - accuracy: 0.9155 - val_loss: 0.3317 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.29653\n",
      "Epoch 399/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.3184 - accuracy: 0.9074 - val_loss: 0.3298 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.29653\n",
      "Epoch 400/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2794 - accuracy: 0.9147 - val_loss: 0.3431 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.29653\n",
      "Epoch 401/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2830 - accuracy: 0.9082 - val_loss: 0.3084 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.29653\n",
      "Epoch 402/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2867 - accuracy: 0.9147 - val_loss: 0.3346 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.29653\n",
      "Epoch 403/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2626 - accuracy: 0.9198 - val_loss: 0.3193 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.29653\n",
      "Epoch 404/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2675 - accuracy: 0.9148 - val_loss: 0.3344 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.29653\n",
      "Epoch 405/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2822 - accuracy: 0.9145 - val_loss: 0.3245 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.29653\n",
      "Epoch 406/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2700 - accuracy: 0.9161 - val_loss: 0.3131 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.29653\n",
      "Epoch 407/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2676 - accuracy: 0.9211 - val_loss: 0.3033 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.29653\n",
      "Epoch 408/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2527 - accuracy: 0.9200 - val_loss: 0.3369 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.29653\n",
      "Epoch 409/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2484 - accuracy: 0.9224 - val_loss: 0.3561 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.29653\n",
      "Epoch 410/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2714 - accuracy: 0.9177 - val_loss: 0.3336 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.29653\n",
      "Epoch 411/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2765 - accuracy: 0.9154 - val_loss: 0.3272 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.29653\n",
      "Epoch 412/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2746 - accuracy: 0.9150 - val_loss: 0.3167 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.29653\n",
      "Epoch 413/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2503 - accuracy: 0.9214 - val_loss: 0.3487 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.29653\n",
      "Epoch 414/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2677 - accuracy: 0.9154 - val_loss: 0.3320 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.29653\n",
      "Epoch 415/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2629 - accuracy: 0.9243 - val_loss: 0.3439 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.29653\n",
      "Epoch 416/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2821 - accuracy: 0.9151 - val_loss: 0.3394 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.29653\n",
      "Epoch 417/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2916 - accuracy: 0.9105 - val_loss: 0.3346 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.29653\n",
      "Epoch 418/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2757 - accuracy: 0.9142 - val_loss: 0.3286 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.29653\n",
      "Epoch 419/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2574 - accuracy: 0.9234 - val_loss: 0.3248 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.29653\n",
      "Epoch 420/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2773 - accuracy: 0.9144 - val_loss: 0.3405 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.29653\n",
      "Epoch 421/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2573 - accuracy: 0.9195 - val_loss: 0.3220 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.29653\n",
      "Epoch 422/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2988 - accuracy: 0.9094 - val_loss: 0.3518 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.29653\n",
      "Epoch 423/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.9144 - val_loss: 0.3342 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.29653\n",
      "Epoch 424/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2838 - accuracy: 0.9151 - val_loss: 0.3208 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.29653\n",
      "Epoch 425/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2770 - accuracy: 0.9181 - val_loss: 0.3284 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.29653\n",
      "Epoch 426/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2650 - accuracy: 0.9193 - val_loss: 0.2907 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00426: val_loss improved from 0.29653 to 0.29067, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 427/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2649 - accuracy: 0.9183 - val_loss: 0.3184 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.29067\n",
      "Epoch 428/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2761 - accuracy: 0.9137 - val_loss: 0.3031 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.29067\n",
      "Epoch 429/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2575 - accuracy: 0.9227 - val_loss: 0.3165 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.29067\n",
      "Epoch 430/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2857 - accuracy: 0.9170 - val_loss: 0.3257 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.29067\n",
      "Epoch 431/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2843 - accuracy: 0.9184 - val_loss: 0.3102 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.29067\n",
      "Epoch 432/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.9168 - val_loss: 0.3134 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.29067\n",
      "Epoch 433/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2594 - accuracy: 0.9201 - val_loss: 0.3042 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.29067\n",
      "Epoch 434/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2467 - accuracy: 0.9257 - val_loss: 0.3103 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.29067\n",
      "Epoch 435/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2876 - accuracy: 0.9130 - val_loss: 0.3200 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.29067\n",
      "Epoch 436/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2671 - accuracy: 0.9190 - val_loss: 0.3094 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.29067\n",
      "Epoch 437/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2609 - accuracy: 0.9170 - val_loss: 0.3129 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.29067\n",
      "Epoch 438/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2526 - accuracy: 0.9241 - val_loss: 0.3375 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.29067\n",
      "Epoch 439/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2533 - accuracy: 0.9224 - val_loss: 0.3161 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.29067\n",
      "Epoch 440/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2466 - accuracy: 0.9280 - val_loss: 0.3331 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.29067\n",
      "Epoch 441/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2716 - accuracy: 0.9194 - val_loss: 0.3239 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.29067\n",
      "Epoch 442/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2707 - accuracy: 0.9184 - val_loss: 0.3319 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.29067\n",
      "Epoch 443/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2599 - accuracy: 0.9171 - val_loss: 0.3385 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.29067\n",
      "Epoch 444/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2686 - accuracy: 0.9204 - val_loss: 0.3287 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.29067\n",
      "Epoch 445/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2736 - accuracy: 0.9155 - val_loss: 0.3451 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.29067\n",
      "Epoch 446/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2615 - accuracy: 0.9174 - val_loss: 0.3613 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.29067\n",
      "Epoch 447/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.9125 - val_loss: 0.3386 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.29067\n",
      "Epoch 448/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2589 - accuracy: 0.9174 - val_loss: 0.3618 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.29067\n",
      "Epoch 449/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.9160 - val_loss: 0.3715 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.29067\n",
      "Epoch 450/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.9147 - val_loss: 0.3270 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.29067\n",
      "Epoch 451/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2527 - accuracy: 0.9174 - val_loss: 0.3306 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.29067\n",
      "Epoch 452/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2676 - accuracy: 0.9203 - val_loss: 0.3321 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.29067\n",
      "Epoch 453/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2732 - accuracy: 0.9158 - val_loss: 0.3965 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.29067\n",
      "Epoch 454/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.9184 - val_loss: 0.3544 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.29067\n",
      "Epoch 455/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2662 - accuracy: 0.9184 - val_loss: 0.3535 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.29067\n",
      "Epoch 456/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2611 - accuracy: 0.9194 - val_loss: 0.3603 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.29067\n",
      "Epoch 457/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.9147 - val_loss: 0.3314 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.29067\n",
      "Epoch 458/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.9180 - val_loss: 0.3518 - val_accuracy: 0.9033\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.29067\n",
      "Epoch 459/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2691 - accuracy: 0.9177 - val_loss: 0.3211 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.29067\n",
      "Epoch 460/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2647 - accuracy: 0.9193 - val_loss: 0.3542 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.29067\n",
      "Epoch 461/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2500 - accuracy: 0.9220 - val_loss: 0.3130 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.29067\n",
      "Epoch 462/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2453 - accuracy: 0.9246 - val_loss: 0.3254 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.29067\n",
      "Epoch 463/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2637 - accuracy: 0.9194 - val_loss: 0.3256 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.29067\n",
      "Epoch 464/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2634 - accuracy: 0.9198 - val_loss: 0.3258 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.29067\n",
      "Epoch 465/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2892 - accuracy: 0.9122 - val_loss: 0.3470 - val_accuracy: 0.8998\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.29067\n",
      "Epoch 466/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.9227 - val_loss: 0.3449 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.29067\n",
      "Epoch 467/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2874 - accuracy: 0.9141 - val_loss: 0.3408 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.29067\n",
      "Epoch 468/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2529 - accuracy: 0.9227 - val_loss: 0.3290 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.29067\n",
      "Epoch 469/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2517 - accuracy: 0.9227 - val_loss: 0.3448 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.29067\n",
      "Epoch 470/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.9203 - val_loss: 0.3395 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.29067\n",
      "Epoch 471/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2730 - accuracy: 0.9204 - val_loss: 0.3645 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.29067\n",
      "Epoch 472/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.9198 - val_loss: 0.3290 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.29067\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2411 - accuracy: 0.9236 - val_loss: 0.3498 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.29067\n",
      "Epoch 474/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2542 - accuracy: 0.9201 - val_loss: 0.3412 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.29067\n",
      "Epoch 475/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2548 - accuracy: 0.9204 - val_loss: 0.3407 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.29067\n",
      "Epoch 476/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.9195 - val_loss: 0.3400 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.29067\n",
      "Epoch 477/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2446 - accuracy: 0.9218 - val_loss: 0.3337 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.29067\n",
      "Epoch 478/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2583 - accuracy: 0.9228 - val_loss: 0.3421 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.29067\n",
      "Epoch 479/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2462 - accuracy: 0.9221 - val_loss: 0.3540 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.29067\n",
      "Epoch 480/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2866 - accuracy: 0.9161 - val_loss: 0.3468 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.29067\n",
      "Epoch 481/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2617 - accuracy: 0.9184 - val_loss: 0.3148 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.29067\n",
      "Epoch 482/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2568 - accuracy: 0.9268 - val_loss: 0.3152 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.29067\n",
      "Epoch 483/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2379 - accuracy: 0.9225 - val_loss: 0.3196 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.29067\n",
      "Epoch 484/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2838 - accuracy: 0.9154 - val_loss: 0.3439 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.29067\n",
      "Epoch 485/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2526 - accuracy: 0.9173 - val_loss: 0.3293 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.29067\n",
      "Epoch 486/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2673 - accuracy: 0.9167 - val_loss: 0.3316 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.29067\n",
      "Epoch 487/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2492 - accuracy: 0.9207 - val_loss: 0.3304 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.29067\n",
      "Epoch 488/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.9200 - val_loss: 0.3401 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.29067\n",
      "Epoch 489/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2652 - accuracy: 0.9194 - val_loss: 0.3130 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.29067\n",
      "Epoch 490/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.9224 - val_loss: 0.3164 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.29067\n",
      "Epoch 491/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.9157 - val_loss: 0.3053 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.29067\n",
      "Epoch 492/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2447 - accuracy: 0.9257 - val_loss: 0.3533 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.29067\n",
      "Epoch 493/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2400 - accuracy: 0.9250 - val_loss: 0.3416 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.29067\n",
      "Epoch 494/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2490 - accuracy: 0.9241 - val_loss: 0.3135 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.29067\n",
      "Epoch 495/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2664 - accuracy: 0.9203 - val_loss: 0.3546 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.29067\n",
      "Epoch 496/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2462 - accuracy: 0.9260 - val_loss: 0.3297 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.29067\n",
      "Epoch 497/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2655 - accuracy: 0.9234 - val_loss: 0.3155 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.29067\n",
      "Epoch 498/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2460 - accuracy: 0.9237 - val_loss: 0.3251 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.29067\n",
      "Epoch 499/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2838 - accuracy: 0.9141 - val_loss: 0.3119 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.29067\n",
      "Epoch 500/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2619 - accuracy: 0.9201 - val_loss: 0.3223 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.29067\n",
      "Epoch 501/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2459 - accuracy: 0.9261 - val_loss: 0.3178 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.29067\n",
      "Epoch 502/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2786 - accuracy: 0.9128 - val_loss: 0.3471 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.29067\n",
      "Epoch 503/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2645 - accuracy: 0.9213 - val_loss: 0.3481 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.29067\n",
      "Epoch 504/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2540 - accuracy: 0.9224 - val_loss: 0.3207 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.29067\n",
      "Epoch 505/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2388 - accuracy: 0.9240 - val_loss: 0.3330 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.29067\n",
      "Epoch 506/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2557 - accuracy: 0.9243 - val_loss: 0.3218 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.29067\n",
      "Epoch 507/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.9251 - val_loss: 0.3380 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.29067\n",
      "Epoch 508/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2644 - accuracy: 0.9270 - val_loss: 0.3180 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.29067\n",
      "Epoch 509/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2703 - accuracy: 0.9227 - val_loss: 0.3238 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.29067\n",
      "Epoch 510/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2244 - accuracy: 0.9273 - val_loss: 0.3068 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.29067\n",
      "Epoch 511/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2494 - accuracy: 0.9257 - val_loss: 0.3285 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.29067\n",
      "Epoch 512/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2395 - accuracy: 0.9267 - val_loss: 0.3421 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.29067\n",
      "Epoch 513/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2244 - accuracy: 0.9287 - val_loss: 0.3459 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.29067\n",
      "Epoch 514/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2490 - accuracy: 0.9307 - val_loss: 0.3406 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.29067\n",
      "Epoch 515/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2485 - accuracy: 0.9256 - val_loss: 0.3210 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.29067\n",
      "Epoch 516/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.9218 - val_loss: 0.3424 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.29067\n",
      "Epoch 517/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2568 - accuracy: 0.9204 - val_loss: 0.3286 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.29067\n",
      "Epoch 518/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.9203 - val_loss: 0.3407 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.29067\n",
      "Epoch 519/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2482 - accuracy: 0.9221 - val_loss: 0.3381 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.29067\n",
      "Epoch 520/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.9174 - val_loss: 0.3369 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.29067\n",
      "Epoch 521/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.9214 - val_loss: 0.3483 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.29067\n",
      "Epoch 522/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.9218 - val_loss: 0.3281 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.29067\n",
      "Epoch 523/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.9258 - val_loss: 0.3403 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.29067\n",
      "Epoch 524/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2478 - accuracy: 0.9213 - val_loss: 0.3405 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.29067\n",
      "Epoch 525/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2308 - accuracy: 0.9293 - val_loss: 0.3308 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.29067\n",
      "Epoch 526/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.9251 - val_loss: 0.3478 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.29067\n",
      "Epoch 527/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2737 - accuracy: 0.9208 - val_loss: 0.3197 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.29067\n",
      "Epoch 528/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2310 - accuracy: 0.9296 - val_loss: 0.3391 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.29067\n",
      "Epoch 529/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2350 - accuracy: 0.9277 - val_loss: 0.3465 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.29067\n",
      "Epoch 530/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2474 - accuracy: 0.9246 - val_loss: 0.3651 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.29067\n",
      "Epoch 531/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2578 - accuracy: 0.9234 - val_loss: 0.3653 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.29067\n",
      "Epoch 532/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2366 - accuracy: 0.9310 - val_loss: 0.3843 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.29067\n",
      "Epoch 533/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2342 - accuracy: 0.9291 - val_loss: 0.3653 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.29067\n",
      "Epoch 534/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2392 - accuracy: 0.9251 - val_loss: 0.3559 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.29067\n",
      "Epoch 535/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2676 - accuracy: 0.9217 - val_loss: 0.3437 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.29067\n",
      "Epoch 536/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2546 - accuracy: 0.9256 - val_loss: 0.3565 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.29067\n",
      "Epoch 537/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2408 - accuracy: 0.9264 - val_loss: 0.3662 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.29067\n",
      "Epoch 538/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2462 - accuracy: 0.9234 - val_loss: 0.3758 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.29067\n",
      "Epoch 539/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2588 - accuracy: 0.9266 - val_loss: 0.3429 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.29067\n",
      "Epoch 540/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2326 - accuracy: 0.9284 - val_loss: 0.3763 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.29067\n",
      "Epoch 541/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2640 - accuracy: 0.9201 - val_loss: 0.3574 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.29067\n",
      "Epoch 542/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2354 - accuracy: 0.9266 - val_loss: 0.3516 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.29067\n",
      "Epoch 543/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2680 - accuracy: 0.9207 - val_loss: 0.3768 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.29067\n",
      "Epoch 544/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2490 - accuracy: 0.9263 - val_loss: 0.3388 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.29067\n",
      "Epoch 545/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2355 - accuracy: 0.9293 - val_loss: 0.3386 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.29067\n",
      "Epoch 546/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2300 - accuracy: 0.9250 - val_loss: 0.3391 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.29067\n",
      "Epoch 547/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2424 - accuracy: 0.9278 - val_loss: 0.3317 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.29067\n",
      "Epoch 548/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2565 - accuracy: 0.9205 - val_loss: 0.3297 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.29067\n",
      "Epoch 549/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.9225 - val_loss: 0.3334 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.29067\n",
      "Epoch 550/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2416 - accuracy: 0.9267 - val_loss: 0.3315 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.29067\n",
      "Epoch 551/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2665 - accuracy: 0.9195 - val_loss: 0.3583 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.29067\n",
      "Epoch 552/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2411 - accuracy: 0.9274 - val_loss: 0.3597 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.29067\n",
      "Epoch 553/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2410 - accuracy: 0.9284 - val_loss: 0.3499 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.29067\n",
      "Epoch 554/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2337 - accuracy: 0.9290 - val_loss: 0.3591 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.29067\n",
      "Epoch 555/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2531 - accuracy: 0.9264 - val_loss: 0.3529 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.29067\n",
      "Epoch 556/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2381 - accuracy: 0.9314 - val_loss: 0.3621 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.29067\n",
      "Epoch 557/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2534 - accuracy: 0.9280 - val_loss: 0.3327 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.29067\n",
      "Epoch 558/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2498 - accuracy: 0.9267 - val_loss: 0.3323 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.29067\n",
      "Epoch 559/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.9195 - val_loss: 0.3410 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.29067\n",
      "Epoch 560/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.9277 - val_loss: 0.3580 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.29067\n",
      "Epoch 561/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2559 - accuracy: 0.9244 - val_loss: 0.3356 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.29067\n",
      "Epoch 562/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2485 - accuracy: 0.9246 - val_loss: 0.3259 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.29067\n",
      "Epoch 563/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.9250 - val_loss: 0.3310 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.29067\n",
      "Epoch 564/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.9228 - val_loss: 0.3572 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.29067\n",
      "Epoch 565/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2521 - accuracy: 0.9248 - val_loss: 0.3194 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.29067\n",
      "Epoch 566/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2422 - accuracy: 0.9296 - val_loss: 0.3467 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.29067\n",
      "Epoch 567/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2481 - accuracy: 0.9254 - val_loss: 0.3112 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.29067\n",
      "Epoch 568/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2417 - accuracy: 0.9258 - val_loss: 0.3211 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.29067\n",
      "Epoch 569/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2419 - accuracy: 0.9313 - val_loss: 0.3202 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.29067\n",
      "Epoch 570/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.9251 - val_loss: 0.3386 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.29067\n",
      "Epoch 571/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.9260 - val_loss: 0.3057 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.29067\n",
      "Epoch 572/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2366 - accuracy: 0.9297 - val_loss: 0.3253 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.29067\n",
      "Epoch 573/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.9243 - val_loss: 0.3325 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.29067\n",
      "Epoch 574/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9281 - val_loss: 0.3307 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.29067\n",
      "Epoch 575/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2458 - accuracy: 0.9231 - val_loss: 0.3242 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.29067\n",
      "Epoch 576/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2542 - accuracy: 0.9225 - val_loss: 0.3140 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.29067\n",
      "Epoch 577/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2562 - accuracy: 0.9184 - val_loss: 0.3237 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.29067\n",
      "Epoch 578/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2474 - accuracy: 0.9264 - val_loss: 0.3000 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.29067\n",
      "Epoch 579/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2225 - accuracy: 0.9309 - val_loss: 0.3088 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.29067\n",
      "Epoch 580/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2548 - accuracy: 0.9248 - val_loss: 0.3185 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.29067\n",
      "Epoch 581/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2387 - accuracy: 0.9311 - val_loss: 0.3019 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.29067\n",
      "Epoch 582/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2587 - accuracy: 0.9266 - val_loss: 0.3068 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.29067\n",
      "Epoch 583/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2247 - accuracy: 0.9298 - val_loss: 0.3078 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.29067\n",
      "Epoch 584/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2412 - accuracy: 0.9274 - val_loss: 0.2998 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.29067\n",
      "Epoch 585/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2397 - accuracy: 0.9288 - val_loss: 0.3179 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.29067\n",
      "Epoch 586/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2331 - accuracy: 0.9286 - val_loss: 0.3136 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.29067\n",
      "Epoch 587/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2551 - accuracy: 0.9247 - val_loss: 0.3117 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.29067\n",
      "Epoch 588/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2536 - accuracy: 0.9260 - val_loss: 0.2988 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.29067\n",
      "Epoch 589/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2515 - accuracy: 0.9257 - val_loss: 0.2908 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.29067\n",
      "Epoch 590/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2437 - accuracy: 0.9260 - val_loss: 0.3061 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.29067\n",
      "Epoch 591/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2329 - accuracy: 0.9290 - val_loss: 0.3362 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.29067\n",
      "Epoch 592/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2439 - accuracy: 0.9280 - val_loss: 0.3335 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.29067\n",
      "Epoch 593/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2364 - accuracy: 0.9276 - val_loss: 0.3215 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.29067\n",
      "Epoch 594/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2487 - accuracy: 0.9221 - val_loss: 0.3233 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.29067\n",
      "Epoch 595/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2509 - accuracy: 0.9257 - val_loss: 0.3117 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.29067\n",
      "Epoch 596/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2499 - accuracy: 0.9260 - val_loss: 0.3211 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.29067\n",
      "Epoch 597/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2250 - accuracy: 0.9329 - val_loss: 0.3526 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.29067\n",
      "Epoch 598/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2627 - accuracy: 0.9238 - val_loss: 0.3254 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.29067\n",
      "Epoch 599/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2435 - accuracy: 0.9241 - val_loss: 0.3296 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.29067\n",
      "Epoch 600/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2188 - accuracy: 0.9359 - val_loss: 0.3438 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.29067\n",
      "Epoch 601/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2338 - accuracy: 0.9296 - val_loss: 0.3190 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.29067\n",
      "Epoch 602/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2312 - accuracy: 0.9267 - val_loss: 0.3541 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.29067\n",
      "Epoch 603/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2054 - accuracy: 0.9353 - val_loss: 0.3539 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.29067\n",
      "Epoch 604/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2428 - accuracy: 0.9273 - val_loss: 0.3539 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.29067\n",
      "Epoch 605/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2501 - accuracy: 0.9253 - val_loss: 0.3454 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.29067\n",
      "Epoch 606/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2312 - accuracy: 0.9301 - val_loss: 0.3393 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.29067\n",
      "Epoch 607/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2497 - accuracy: 0.9250 - val_loss: 0.3434 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.29067\n",
      "Epoch 608/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2532 - accuracy: 0.9237 - val_loss: 0.3218 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.29067\n",
      "Epoch 609/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2271 - accuracy: 0.9253 - val_loss: 0.3301 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.29067\n",
      "Epoch 610/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2319 - accuracy: 0.9344 - val_loss: 0.3517 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.29067\n",
      "Epoch 611/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2343 - accuracy: 0.9273 - val_loss: 0.3365 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.29067\n",
      "Epoch 612/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2327 - accuracy: 0.9290 - val_loss: 0.3523 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.29067\n",
      "Epoch 613/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2505 - accuracy: 0.9280 - val_loss: 0.3428 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.29067\n",
      "Epoch 614/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2675 - accuracy: 0.9233 - val_loss: 0.3360 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.29067\n",
      "Epoch 615/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2403 - accuracy: 0.9294 - val_loss: 0.3258 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.29067\n",
      "Epoch 616/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2643 - accuracy: 0.9208 - val_loss: 0.3606 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.29067\n",
      "Epoch 617/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2421 - accuracy: 0.9270 - val_loss: 0.3522 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.29067\n",
      "Epoch 618/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2558 - accuracy: 0.9253 - val_loss: 0.3549 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.29067\n",
      "Epoch 619/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2360 - accuracy: 0.9296 - val_loss: 0.3523 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.29067\n",
      "Epoch 620/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2498 - accuracy: 0.9260 - val_loss: 0.3414 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.29067\n",
      "Epoch 621/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2540 - accuracy: 0.9254 - val_loss: 0.3574 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.29067\n",
      "Epoch 622/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2277 - accuracy: 0.9281 - val_loss: 0.3278 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.29067\n",
      "Epoch 623/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2236 - accuracy: 0.9329 - val_loss: 0.3829 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.29067\n",
      "Epoch 624/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2628 - accuracy: 0.9227 - val_loss: 0.3741 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.29067\n",
      "Epoch 625/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2391 - accuracy: 0.9248 - val_loss: 0.3742 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.29067\n",
      "Epoch 626/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2391 - accuracy: 0.9296 - val_loss: 0.3600 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.29067\n",
      "Epoch 627/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2428 - accuracy: 0.9270 - val_loss: 0.3412 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.29067\n",
      "Epoch 628/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2340 - accuracy: 0.9291 - val_loss: 0.3108 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.29067\n",
      "Epoch 629/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2563 - accuracy: 0.9241 - val_loss: 0.3423 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.29067\n",
      "Epoch 630/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2536 - accuracy: 0.9286 - val_loss: 0.3381 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.29067\n",
      "Epoch 631/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2594 - accuracy: 0.9274 - val_loss: 0.3423 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.29067\n",
      "Epoch 632/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2144 - accuracy: 0.9331 - val_loss: 0.3527 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.29067\n",
      "Epoch 633/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2267 - accuracy: 0.9306 - val_loss: 0.3489 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.29067\n",
      "Epoch 634/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2454 - accuracy: 0.9301 - val_loss: 0.3123 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.29067\n",
      "Epoch 635/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2485 - accuracy: 0.9283 - val_loss: 0.3399 - val_accuracy: 0.9027\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.29067\n",
      "Epoch 636/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2573 - accuracy: 0.9240 - val_loss: 0.3305 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.29067\n",
      "Epoch 637/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2395 - accuracy: 0.9309 - val_loss: 0.3465 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.29067\n",
      "Epoch 638/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2484 - accuracy: 0.9280 - val_loss: 0.3561 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.29067\n",
      "Epoch 639/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2596 - accuracy: 0.9224 - val_loss: 0.3291 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.29067\n",
      "Epoch 640/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2411 - accuracy: 0.9294 - val_loss: 0.3392 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.29067\n",
      "Epoch 641/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2307 - accuracy: 0.9314 - val_loss: 0.3482 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.29067\n",
      "Epoch 642/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2327 - accuracy: 0.9294 - val_loss: 0.3546 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.29067\n",
      "Epoch 643/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2276 - accuracy: 0.9331 - val_loss: 0.3490 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.29067\n",
      "Epoch 644/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2293 - accuracy: 0.9324 - val_loss: 0.3416 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.29067\n",
      "Epoch 645/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2432 - accuracy: 0.9253 - val_loss: 0.3372 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.29067\n",
      "Epoch 646/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2639 - accuracy: 0.9251 - val_loss: 0.3652 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.29067\n",
      "Epoch 647/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2372 - accuracy: 0.9283 - val_loss: 0.3456 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.29067\n",
      "Epoch 648/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2324 - accuracy: 0.9290 - val_loss: 0.3766 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.29067\n",
      "Epoch 649/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2500 - accuracy: 0.9231 - val_loss: 0.3336 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.29067\n",
      "Epoch 650/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2302 - accuracy: 0.9286 - val_loss: 0.3535 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.29067\n",
      "Epoch 651/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2476 - accuracy: 0.9270 - val_loss: 0.3363 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.29067\n",
      "Epoch 652/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2429 - accuracy: 0.9307 - val_loss: 0.3480 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.29067\n",
      "Epoch 653/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2181 - accuracy: 0.9323 - val_loss: 0.3370 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.29067\n",
      "Epoch 654/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2466 - accuracy: 0.9290 - val_loss: 0.3744 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.29067\n",
      "Epoch 655/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2517 - accuracy: 0.9250 - val_loss: 0.3476 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.29067\n",
      "Epoch 656/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2344 - accuracy: 0.9278 - val_loss: 0.3732 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.29067\n",
      "Epoch 657/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2317 - accuracy: 0.9276 - val_loss: 0.3550 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.29067\n",
      "Epoch 658/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2336 - accuracy: 0.9323 - val_loss: 0.3566 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.29067\n",
      "Epoch 659/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2510 - accuracy: 0.9261 - val_loss: 0.3396 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.29067\n",
      "Epoch 660/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2620 - accuracy: 0.9207 - val_loss: 0.3531 - val_accuracy: 0.9038\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.29067\n",
      "Epoch 661/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2152 - accuracy: 0.9326 - val_loss: 0.3468 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.29067\n",
      "Epoch 662/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2248 - accuracy: 0.9339 - val_loss: 0.3402 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.29067\n",
      "Epoch 663/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2269 - accuracy: 0.9311 - val_loss: 0.3887 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.29067\n",
      "Epoch 664/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2537 - accuracy: 0.9273 - val_loss: 0.3634 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.29067\n",
      "Epoch 665/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2627 - accuracy: 0.9283 - val_loss: 0.3656 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.29067\n",
      "Epoch 666/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2484 - accuracy: 0.9290 - val_loss: 0.3607 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.29067\n",
      "Epoch 667/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2170 - accuracy: 0.9376 - val_loss: 0.3628 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.29067\n",
      "Epoch 668/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2167 - accuracy: 0.9354 - val_loss: 0.3479 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.29067\n",
      "Epoch 669/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2421 - accuracy: 0.9278 - val_loss: 0.3585 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.29067\n",
      "Epoch 670/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2390 - accuracy: 0.9314 - val_loss: 0.3634 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.29067\n",
      "Epoch 671/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2481 - accuracy: 0.9296 - val_loss: 0.3444 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.29067\n",
      "Epoch 672/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2270 - accuracy: 0.9311 - val_loss: 0.3308 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.29067\n",
      "Epoch 673/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2157 - accuracy: 0.9323 - val_loss: 0.3621 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.29067\n",
      "Epoch 674/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.9257 - val_loss: 0.3375 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.29067\n",
      "Epoch 675/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2500 - accuracy: 0.9244 - val_loss: 0.3387 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.29067\n",
      "Epoch 676/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2449 - accuracy: 0.9307 - val_loss: 0.3262 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.29067\n",
      "Epoch 677/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2491 - accuracy: 0.9330 - val_loss: 0.3381 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.29067\n",
      "Epoch 678/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2346 - accuracy: 0.9276 - val_loss: 0.3345 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.29067\n",
      "Epoch 679/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9303 - val_loss: 0.3394 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.29067\n",
      "Epoch 680/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2326 - accuracy: 0.9344 - val_loss: 0.3512 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.29067\n",
      "Epoch 681/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2172 - accuracy: 0.9339 - val_loss: 0.3502 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.29067\n",
      "Epoch 682/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2462 - accuracy: 0.9264 - val_loss: 0.3387 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.29067\n",
      "Epoch 683/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2239 - accuracy: 0.9331 - val_loss: 0.3535 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.29067\n",
      "Epoch 684/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2210 - accuracy: 0.9364 - val_loss: 0.3423 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.29067\n",
      "Epoch 685/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2377 - accuracy: 0.9331 - val_loss: 0.3570 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.29067\n",
      "Epoch 686/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2434 - accuracy: 0.9267 - val_loss: 0.3217 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.29067\n",
      "Epoch 687/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2188 - accuracy: 0.9330 - val_loss: 0.3294 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.29067\n",
      "Epoch 688/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2283 - accuracy: 0.9333 - val_loss: 0.3536 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.29067\n",
      "Epoch 689/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2433 - accuracy: 0.9281 - val_loss: 0.3385 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.29067\n",
      "Epoch 690/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2338 - accuracy: 0.9296 - val_loss: 0.3355 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.29067\n",
      "Epoch 691/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2217 - accuracy: 0.9331 - val_loss: 0.3460 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.29067\n",
      "Epoch 692/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2293 - accuracy: 0.9324 - val_loss: 0.3366 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.29067\n",
      "Epoch 693/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2432 - accuracy: 0.9246 - val_loss: 0.3463 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.29067\n",
      "Epoch 694/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2410 - accuracy: 0.9280 - val_loss: 0.3480 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.29067\n",
      "Epoch 695/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2406 - accuracy: 0.9319 - val_loss: 0.3310 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.29067\n",
      "Epoch 696/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2202 - accuracy: 0.9331 - val_loss: 0.3034 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.29067\n",
      "Epoch 697/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2527 - accuracy: 0.9294 - val_loss: 0.3085 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.29067\n",
      "Epoch 698/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2372 - accuracy: 0.9281 - val_loss: 0.3370 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.29067\n",
      "Epoch 699/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2365 - accuracy: 0.9293 - val_loss: 0.3308 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.29067\n",
      "Epoch 700/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2300 - accuracy: 0.9306 - val_loss: 0.3310 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.29067\n",
      "Epoch 701/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2276 - accuracy: 0.9330 - val_loss: 0.3475 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.29067\n",
      "Epoch 702/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2138 - accuracy: 0.9380 - val_loss: 0.3343 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.29067\n",
      "Epoch 703/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2145 - accuracy: 0.9373 - val_loss: 0.3158 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.29067\n",
      "Epoch 704/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2461 - accuracy: 0.9336 - val_loss: 0.3234 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.29067\n",
      "Epoch 705/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2334 - accuracy: 0.9313 - val_loss: 0.3122 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.29067\n",
      "Epoch 706/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2331 - accuracy: 0.9317 - val_loss: 0.3194 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.29067\n",
      "Epoch 707/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2347 - accuracy: 0.9324 - val_loss: 0.3154 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.29067\n",
      "Epoch 708/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2209 - accuracy: 0.9317 - val_loss: 0.3298 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.29067\n",
      "Epoch 709/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2281 - accuracy: 0.9343 - val_loss: 0.3567 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.29067\n",
      "Epoch 710/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2300 - accuracy: 0.9278 - val_loss: 0.3520 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.29067\n",
      "Epoch 711/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2416 - accuracy: 0.9307 - val_loss: 0.3418 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.29067\n",
      "Epoch 712/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2322 - accuracy: 0.9304 - val_loss: 0.3297 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.29067\n",
      "Epoch 713/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2501 - accuracy: 0.9261 - val_loss: 0.3210 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.29067\n",
      "Epoch 714/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2147 - accuracy: 0.9337 - val_loss: 0.3251 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.29067\n",
      "Epoch 715/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2133 - accuracy: 0.9323 - val_loss: 0.3001 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.29067\n",
      "Epoch 716/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2340 - accuracy: 0.9327 - val_loss: 0.3253 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.29067\n",
      "Epoch 717/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2334 - accuracy: 0.9320 - val_loss: 0.3232 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.29067\n",
      "Epoch 718/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2298 - accuracy: 0.9320 - val_loss: 0.3204 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.29067\n",
      "Epoch 719/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2451 - accuracy: 0.9287 - val_loss: 0.3378 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.29067\n",
      "Epoch 720/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2305 - accuracy: 0.9329 - val_loss: 0.3414 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.29067\n",
      "Epoch 721/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2348 - accuracy: 0.9303 - val_loss: 0.3565 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.29067\n",
      "Epoch 722/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2264 - accuracy: 0.9374 - val_loss: 0.3405 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.29067\n",
      "Epoch 723/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2170 - accuracy: 0.9413 - val_loss: 0.3271 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.29067\n",
      "Epoch 724/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2298 - accuracy: 0.9307 - val_loss: 0.3343 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.29067\n",
      "Epoch 725/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2322 - accuracy: 0.9321 - val_loss: 0.3466 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.29067\n",
      "Epoch 726/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2502 - accuracy: 0.9287 - val_loss: 0.3595 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.29067\n",
      "Epoch 727/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9336 - val_loss: 0.3296 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.29067\n",
      "Epoch 728/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2065 - accuracy: 0.9404 - val_loss: 0.3357 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.29067\n",
      "Epoch 729/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2326 - accuracy: 0.9313 - val_loss: 0.3742 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.29067\n",
      "Epoch 730/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2371 - accuracy: 0.9353 - val_loss: 0.3446 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.29067\n",
      "Epoch 731/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2521 - accuracy: 0.9261 - val_loss: 0.3263 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.29067\n",
      "Epoch 732/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2145 - accuracy: 0.9389 - val_loss: 0.3817 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.29067\n",
      "Epoch 733/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2307 - accuracy: 0.9340 - val_loss: 0.3670 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.29067\n",
      "Epoch 734/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2111 - accuracy: 0.9347 - val_loss: 0.3666 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.29067\n",
      "Epoch 735/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2225 - accuracy: 0.9307 - val_loss: 0.3503 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.29067\n",
      "Epoch 736/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2241 - accuracy: 0.9339 - val_loss: 0.3746 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.29067\n",
      "Epoch 737/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2267 - accuracy: 0.9339 - val_loss: 0.3402 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.29067\n",
      "Epoch 738/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2374 - accuracy: 0.9300 - val_loss: 0.3209 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.29067\n",
      "Epoch 739/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2336 - accuracy: 0.9300 - val_loss: 0.3662 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.29067\n",
      "Epoch 740/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2364 - accuracy: 0.9346 - val_loss: 0.3318 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.29067\n",
      "Epoch 741/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2196 - accuracy: 0.9341 - val_loss: 0.3457 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.29067\n",
      "Epoch 742/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2141 - accuracy: 0.9364 - val_loss: 0.3743 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.29067\n",
      "Epoch 743/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2066 - accuracy: 0.9377 - val_loss: 0.3661 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.29067\n",
      "Epoch 744/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2318 - accuracy: 0.9317 - val_loss: 0.3307 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.29067\n",
      "Epoch 745/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2481 - accuracy: 0.9270 - val_loss: 0.3274 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.29067\n",
      "Epoch 746/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2259 - accuracy: 0.9329 - val_loss: 0.3215 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.29067\n",
      "Epoch 747/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2261 - accuracy: 0.9331 - val_loss: 0.3511 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.29067\n",
      "Epoch 748/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2500 - accuracy: 0.9271 - val_loss: 0.3248 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.29067\n",
      "Epoch 749/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2228 - accuracy: 0.9323 - val_loss: 0.3639 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.29067\n",
      "Epoch 750/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2135 - accuracy: 0.9320 - val_loss: 0.3408 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.29067\n",
      "Epoch 751/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.9301 - val_loss: 0.3427 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.29067\n",
      "Epoch 752/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.9313 - val_loss: 0.3526 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.29067\n",
      "Epoch 753/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2085 - accuracy: 0.9366 - val_loss: 0.3410 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.29067\n",
      "Epoch 754/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2177 - accuracy: 0.9346 - val_loss: 0.3177 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.29067\n",
      "Epoch 755/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2095 - accuracy: 0.9373 - val_loss: 0.3323 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.29067\n",
      "Epoch 756/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2391 - accuracy: 0.9310 - val_loss: 0.3427 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.29067\n",
      "Epoch 757/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2315 - accuracy: 0.9337 - val_loss: 0.3335 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.29067\n",
      "Epoch 758/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2352 - accuracy: 0.9319 - val_loss: 0.3287 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.29067\n",
      "Epoch 759/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2427 - accuracy: 0.9298 - val_loss: 0.3407 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.29067\n",
      "Epoch 760/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2148 - accuracy: 0.9360 - val_loss: 0.3294 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.29067\n",
      "Epoch 761/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2180 - accuracy: 0.9356 - val_loss: 0.3179 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.29067\n",
      "Epoch 762/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2281 - accuracy: 0.9273 - val_loss: 0.3434 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.29067\n",
      "Epoch 763/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2238 - accuracy: 0.9339 - val_loss: 0.3367 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.29067\n",
      "Epoch 764/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2270 - accuracy: 0.9313 - val_loss: 0.3286 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.29067\n",
      "Epoch 765/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2275 - accuracy: 0.9320 - val_loss: 0.3320 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.29067\n",
      "Epoch 766/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2374 - accuracy: 0.9290 - val_loss: 0.3023 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.29067\n",
      "Epoch 767/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2256 - accuracy: 0.9336 - val_loss: 0.3193 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.29067\n",
      "Epoch 768/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2296 - accuracy: 0.9331 - val_loss: 0.3332 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.29067\n",
      "Epoch 769/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2071 - accuracy: 0.9337 - val_loss: 0.3435 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.29067\n",
      "Epoch 770/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2124 - accuracy: 0.9361 - val_loss: 0.3260 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.29067\n",
      "Epoch 771/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2213 - accuracy: 0.9337 - val_loss: 0.3295 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.29067\n",
      "Epoch 772/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2401 - accuracy: 0.9293 - val_loss: 0.3355 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.29067\n",
      "Epoch 773/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2160 - accuracy: 0.9384 - val_loss: 0.3296 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.29067\n",
      "Epoch 774/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2235 - accuracy: 0.9353 - val_loss: 0.3449 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.29067\n",
      "Epoch 775/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2188 - accuracy: 0.9351 - val_loss: 0.3294 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.29067\n",
      "Epoch 776/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2274 - accuracy: 0.9353 - val_loss: 0.3245 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.29067\n",
      "Epoch 777/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2317 - accuracy: 0.9361 - val_loss: 0.3386 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.29067\n",
      "Epoch 778/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2411 - accuracy: 0.9307 - val_loss: 0.3035 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.29067\n",
      "Epoch 779/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2324 - accuracy: 0.9296 - val_loss: 0.3091 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.29067\n",
      "Epoch 780/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2075 - accuracy: 0.9367 - val_loss: 0.3233 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.29067\n",
      "Epoch 781/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2412 - accuracy: 0.9263 - val_loss: 0.3377 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.29067\n",
      "Epoch 782/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2164 - accuracy: 0.9350 - val_loss: 0.3510 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.29067\n",
      "Epoch 783/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2189 - accuracy: 0.9370 - val_loss: 0.3453 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.29067\n",
      "Epoch 784/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2214 - accuracy: 0.9380 - val_loss: 0.3339 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.29067\n",
      "Epoch 785/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2161 - accuracy: 0.9349 - val_loss: 0.3312 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.29067\n",
      "Epoch 786/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2227 - accuracy: 0.9327 - val_loss: 0.3298 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.29067\n",
      "Epoch 787/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2258 - accuracy: 0.9300 - val_loss: 0.3418 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.29067\n",
      "Epoch 788/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2393 - accuracy: 0.9336 - val_loss: 0.3520 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.29067\n",
      "Epoch 789/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2485 - accuracy: 0.9327 - val_loss: 0.3474 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.29067\n",
      "Epoch 790/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2203 - accuracy: 0.9311 - val_loss: 0.3400 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.29067\n",
      "Epoch 791/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2309 - accuracy: 0.9293 - val_loss: 0.3339 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.29067\n",
      "Epoch 792/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2141 - accuracy: 0.9353 - val_loss: 0.3498 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.29067\n",
      "Epoch 793/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2335 - accuracy: 0.9316 - val_loss: 0.3288 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.29067\n",
      "Epoch 794/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2289 - accuracy: 0.9327 - val_loss: 0.3441 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.29067\n",
      "Epoch 795/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2180 - accuracy: 0.9331 - val_loss: 0.3559 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.29067\n",
      "Epoch 796/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2166 - accuracy: 0.9343 - val_loss: 0.3298 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.29067\n",
      "Epoch 797/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2224 - accuracy: 0.9343 - val_loss: 0.3797 - val_accuracy: 0.9050\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.29067\n",
      "Epoch 798/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2297 - accuracy: 0.9354 - val_loss: 0.3328 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.29067\n",
      "Epoch 799/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2470 - accuracy: 0.9317 - val_loss: 0.3299 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.29067\n",
      "Epoch 800/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2088 - accuracy: 0.9374 - val_loss: 0.3351 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.29067\n",
      "Epoch 801/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2258 - accuracy: 0.9366 - val_loss: 0.3444 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.29067\n",
      "Epoch 802/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2390 - accuracy: 0.9337 - val_loss: 0.3284 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.29067\n",
      "Epoch 803/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2310 - accuracy: 0.9294 - val_loss: 0.3346 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.29067\n",
      "Epoch 804/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2415 - accuracy: 0.9291 - val_loss: 0.3445 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.29067\n",
      "Epoch 805/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2235 - accuracy: 0.9341 - val_loss: 0.3599 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.29067\n",
      "Epoch 806/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2316 - accuracy: 0.9351 - val_loss: 0.3256 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.29067\n",
      "Epoch 807/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2290 - accuracy: 0.9357 - val_loss: 0.3700 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.29067\n",
      "Epoch 808/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2183 - accuracy: 0.9321 - val_loss: 0.3403 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.29067\n",
      "Epoch 809/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2247 - accuracy: 0.9329 - val_loss: 0.3397 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.29067\n",
      "Epoch 810/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2276 - accuracy: 0.9340 - val_loss: 0.3549 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.29067\n",
      "Epoch 811/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2065 - accuracy: 0.9354 - val_loss: 0.3537 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.29067\n",
      "Epoch 812/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2349 - accuracy: 0.9341 - val_loss: 0.3340 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.29067\n",
      "Epoch 813/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2170 - accuracy: 0.9354 - val_loss: 0.3392 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.29067\n",
      "Epoch 814/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2333 - accuracy: 0.9301 - val_loss: 0.3232 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.29067\n",
      "Epoch 815/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1897 - accuracy: 0.9416 - val_loss: 0.3708 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.29067\n",
      "Epoch 816/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2180 - accuracy: 0.9396 - val_loss: 0.3561 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.29067\n",
      "Epoch 817/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2152 - accuracy: 0.9341 - val_loss: 0.3260 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.29067\n",
      "Epoch 818/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2567 - accuracy: 0.9258 - val_loss: 0.3455 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.29067\n",
      "Epoch 819/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2225 - accuracy: 0.9329 - val_loss: 0.3513 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.29067\n",
      "Epoch 820/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2210 - accuracy: 0.9366 - val_loss: 0.3371 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.29067\n",
      "Epoch 821/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2311 - accuracy: 0.9319 - val_loss: 0.3554 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.29067\n",
      "Epoch 822/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2176 - accuracy: 0.9346 - val_loss: 0.3387 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.29067\n",
      "Epoch 823/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2025 - accuracy: 0.9386 - val_loss: 0.3442 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.29067\n",
      "Epoch 824/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2264 - accuracy: 0.9384 - val_loss: 0.3434 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.29067\n",
      "Epoch 825/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2135 - accuracy: 0.9380 - val_loss: 0.3309 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.29067\n",
      "Epoch 826/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2049 - accuracy: 0.9384 - val_loss: 0.3634 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.29067\n",
      "Epoch 827/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2087 - accuracy: 0.9379 - val_loss: 0.3564 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.29067\n",
      "Epoch 828/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2142 - accuracy: 0.9387 - val_loss: 0.3370 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.29067\n",
      "Epoch 829/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2184 - accuracy: 0.9383 - val_loss: 0.3334 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.29067\n",
      "Epoch 830/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2089 - accuracy: 0.9380 - val_loss: 0.3563 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.29067\n",
      "Epoch 831/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2149 - accuracy: 0.9374 - val_loss: 0.3658 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.29067\n",
      "Epoch 832/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2152 - accuracy: 0.9373 - val_loss: 0.3343 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.29067\n",
      "Epoch 833/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2186 - accuracy: 0.9376 - val_loss: 0.3458 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.29067\n",
      "Epoch 834/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2240 - accuracy: 0.9359 - val_loss: 0.3612 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.29067\n",
      "Epoch 835/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2325 - accuracy: 0.9350 - val_loss: 0.3658 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.29067\n",
      "Epoch 836/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2296 - accuracy: 0.9337 - val_loss: 0.3668 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.29067\n",
      "Epoch 837/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2310 - accuracy: 0.9343 - val_loss: 0.3188 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.29067\n",
      "Epoch 838/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2111 - accuracy: 0.9343 - val_loss: 0.3422 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.29067\n",
      "Epoch 839/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2200 - accuracy: 0.9363 - val_loss: 0.3477 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.29067\n",
      "Epoch 840/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2406 - accuracy: 0.9281 - val_loss: 0.3591 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.29067\n",
      "Epoch 841/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2310 - accuracy: 0.9337 - val_loss: 0.3323 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.29067\n",
      "Epoch 842/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2140 - accuracy: 0.9364 - val_loss: 0.3290 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.29067\n",
      "Epoch 843/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2132 - accuracy: 0.9366 - val_loss: 0.3550 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.29067\n",
      "Epoch 844/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2161 - accuracy: 0.9349 - val_loss: 0.3326 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.29067\n",
      "Epoch 845/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2192 - accuracy: 0.9372 - val_loss: 0.3408 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.29067\n",
      "Epoch 846/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2048 - accuracy: 0.9396 - val_loss: 0.3529 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.29067\n",
      "Epoch 847/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2280 - accuracy: 0.9377 - val_loss: 0.3387 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.29067\n",
      "Epoch 848/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2461 - accuracy: 0.9286 - val_loss: 0.3509 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.29067\n",
      "Epoch 849/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2214 - accuracy: 0.9350 - val_loss: 0.3657 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.29067\n",
      "Epoch 850/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2297 - accuracy: 0.9407 - val_loss: 0.3704 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.29067\n",
      "Epoch 851/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2262 - accuracy: 0.9324 - val_loss: 0.3491 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.29067\n",
      "Epoch 852/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2432 - accuracy: 0.9341 - val_loss: 0.3380 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.29067\n",
      "Epoch 853/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2219 - accuracy: 0.9340 - val_loss: 0.3683 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.29067\n",
      "Epoch 854/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1993 - accuracy: 0.9396 - val_loss: 0.3389 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.29067\n",
      "Epoch 855/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1932 - accuracy: 0.9435 - val_loss: 0.3575 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.29067\n",
      "Epoch 856/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2121 - accuracy: 0.9413 - val_loss: 0.3658 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.29067\n",
      "Epoch 857/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2092 - accuracy: 0.9377 - val_loss: 0.3688 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.29067\n",
      "Epoch 858/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9301 - val_loss: 0.3170 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.29067\n",
      "Epoch 859/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2118 - accuracy: 0.9353 - val_loss: 0.3313 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.29067\n",
      "Epoch 860/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2107 - accuracy: 0.9412 - val_loss: 0.3460 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.29067\n",
      "Epoch 861/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2353 - accuracy: 0.9366 - val_loss: 0.3657 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.29067\n",
      "Epoch 862/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2222 - accuracy: 0.9361 - val_loss: 0.3506 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.29067\n",
      "Epoch 863/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2145 - accuracy: 0.9349 - val_loss: 0.3562 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.29067\n",
      "Epoch 864/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2229 - accuracy: 0.9343 - val_loss: 0.3300 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.29067\n",
      "Epoch 865/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2202 - accuracy: 0.9349 - val_loss: 0.3222 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.29067\n",
      "Epoch 866/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2147 - accuracy: 0.9359 - val_loss: 0.3576 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.29067\n",
      "Epoch 867/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2392 - accuracy: 0.9339 - val_loss: 0.3531 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.29067\n",
      "Epoch 868/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2102 - accuracy: 0.9370 - val_loss: 0.3424 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.29067\n",
      "Epoch 869/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2087 - accuracy: 0.9344 - val_loss: 0.3563 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.29067\n",
      "Epoch 870/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2246 - accuracy: 0.9389 - val_loss: 0.3299 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.29067\n",
      "Epoch 871/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2169 - accuracy: 0.9351 - val_loss: 0.3407 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.29067\n",
      "Epoch 872/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2475 - accuracy: 0.9294 - val_loss: 0.3261 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.29067\n",
      "Epoch 873/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2041 - accuracy: 0.9344 - val_loss: 0.3438 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.29067\n",
      "Epoch 874/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2066 - accuracy: 0.9366 - val_loss: 0.3344 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.29067\n",
      "Epoch 875/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2276 - accuracy: 0.9363 - val_loss: 0.3651 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.29067\n",
      "Epoch 876/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2135 - accuracy: 0.9414 - val_loss: 0.3546 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.29067\n",
      "Epoch 877/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2222 - accuracy: 0.9346 - val_loss: 0.3469 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.29067\n",
      "Epoch 878/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2217 - accuracy: 0.9372 - val_loss: 0.3677 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.29067\n",
      "Epoch 879/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1808 - accuracy: 0.9453 - val_loss: 0.3603 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.29067\n",
      "Epoch 880/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2043 - accuracy: 0.9366 - val_loss: 0.3763 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.29067\n",
      "Epoch 881/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2347 - accuracy: 0.9334 - val_loss: 0.3467 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.29067\n",
      "Epoch 882/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2385 - accuracy: 0.9317 - val_loss: 0.3418 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.29067\n",
      "Epoch 883/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2363 - accuracy: 0.9287 - val_loss: 0.3610 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.29067\n",
      "Epoch 884/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2285 - accuracy: 0.9333 - val_loss: 0.3327 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.29067\n",
      "Epoch 885/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2167 - accuracy: 0.9350 - val_loss: 0.3492 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.29067\n",
      "Epoch 886/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1947 - accuracy: 0.9399 - val_loss: 0.3513 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.29067\n",
      "Epoch 887/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2222 - accuracy: 0.9400 - val_loss: 0.3434 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.29067\n",
      "Epoch 888/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9324 - val_loss: 0.3511 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.29067\n",
      "Epoch 889/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2002 - accuracy: 0.9430 - val_loss: 0.3616 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.29067\n",
      "Epoch 890/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.9286 - val_loss: 0.3610 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.29067\n",
      "Epoch 891/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2113 - accuracy: 0.9416 - val_loss: 0.3339 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.29067\n",
      "Epoch 892/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2086 - accuracy: 0.9414 - val_loss: 0.3508 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.29067\n",
      "Epoch 893/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2109 - accuracy: 0.9380 - val_loss: 0.3544 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.29067\n",
      "Epoch 894/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2240 - accuracy: 0.9404 - val_loss: 0.3377 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.29067\n",
      "Epoch 895/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1984 - accuracy: 0.9417 - val_loss: 0.3660 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.29067\n",
      "Epoch 896/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2181 - accuracy: 0.9360 - val_loss: 0.3545 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.29067\n",
      "Epoch 897/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2149 - accuracy: 0.9357 - val_loss: 0.3798 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.29067\n",
      "Epoch 898/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2303 - accuracy: 0.9346 - val_loss: 0.3371 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.29067\n",
      "Epoch 899/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2025 - accuracy: 0.9404 - val_loss: 0.3519 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.29067\n",
      "Epoch 900/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2107 - accuracy: 0.9389 - val_loss: 0.3310 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.29067\n",
      "Epoch 901/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2198 - accuracy: 0.9366 - val_loss: 0.3455 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.29067\n",
      "Epoch 902/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1957 - accuracy: 0.9419 - val_loss: 0.3496 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.29067\n",
      "Epoch 903/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2290 - accuracy: 0.9330 - val_loss: 0.3387 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.29067\n",
      "Epoch 904/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2177 - accuracy: 0.9334 - val_loss: 0.3438 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.29067\n",
      "Epoch 905/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2151 - accuracy: 0.9377 - val_loss: 0.3474 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.29067\n",
      "Epoch 906/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2043 - accuracy: 0.9410 - val_loss: 0.3614 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.29067\n",
      "Epoch 907/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2297 - accuracy: 0.9334 - val_loss: 0.3470 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.29067\n",
      "Epoch 908/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2328 - accuracy: 0.9367 - val_loss: 0.3671 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.29067\n",
      "Epoch 909/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2124 - accuracy: 0.9407 - val_loss: 0.3661 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.29067\n",
      "Epoch 910/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2178 - accuracy: 0.9367 - val_loss: 0.3974 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.29067\n",
      "Epoch 911/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2279 - accuracy: 0.9389 - val_loss: 0.3581 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.29067\n",
      "Epoch 912/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1966 - accuracy: 0.9437 - val_loss: 0.3370 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.29067\n",
      "Epoch 913/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2315 - accuracy: 0.9343 - val_loss: 0.3550 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.29067\n",
      "Epoch 914/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2087 - accuracy: 0.9409 - val_loss: 0.3706 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.29067\n",
      "Epoch 915/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2468 - accuracy: 0.9344 - val_loss: 0.4028 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.29067\n",
      "Epoch 916/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2094 - accuracy: 0.9419 - val_loss: 0.3677 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.29067\n",
      "Epoch 917/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1966 - accuracy: 0.9399 - val_loss: 0.3948 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.29067\n",
      "Epoch 918/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2104 - accuracy: 0.9402 - val_loss: 0.3514 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.29067\n",
      "Epoch 919/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2177 - accuracy: 0.9399 - val_loss: 0.3618 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.29067\n",
      "Epoch 920/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2513 - accuracy: 0.9344 - val_loss: 0.3419 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.29067\n",
      "Epoch 921/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2132 - accuracy: 0.9361 - val_loss: 0.3361 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.29067\n",
      "Epoch 922/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2176 - accuracy: 0.9351 - val_loss: 0.3341 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.29067\n",
      "Epoch 923/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2007 - accuracy: 0.9436 - val_loss: 0.3388 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.29067\n",
      "Epoch 924/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2267 - accuracy: 0.9363 - val_loss: 0.3579 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.29067\n",
      "Epoch 925/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2270 - accuracy: 0.9366 - val_loss: 0.3582 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.29067\n",
      "Epoch 926/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9354 - val_loss: 0.3654 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.29067\n",
      "Epoch 927/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2448 - accuracy: 0.9343 - val_loss: 0.3455 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.29067\n",
      "Epoch 928/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2323 - accuracy: 0.9339 - val_loss: 0.3466 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.29067\n",
      "Epoch 929/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2129 - accuracy: 0.9393 - val_loss: 0.3590 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.29067\n",
      "Epoch 930/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2077 - accuracy: 0.9406 - val_loss: 0.3705 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.29067\n",
      "Epoch 931/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2186 - accuracy: 0.9387 - val_loss: 0.3381 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.29067\n",
      "Epoch 932/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2264 - accuracy: 0.9350 - val_loss: 0.3403 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.29067\n",
      "Epoch 933/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2113 - accuracy: 0.9376 - val_loss: 0.3585 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.29067\n",
      "Epoch 934/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2229 - accuracy: 0.9353 - val_loss: 0.3444 - val_accuracy: 0.9073\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.29067\n",
      "Epoch 935/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2127 - accuracy: 0.9373 - val_loss: 0.3615 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.29067\n",
      "Epoch 936/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2173 - accuracy: 0.9347 - val_loss: 0.3424 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.29067\n",
      "Epoch 937/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2134 - accuracy: 0.9373 - val_loss: 0.3381 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.29067\n",
      "Epoch 938/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2183 - accuracy: 0.9417 - val_loss: 0.3411 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.29067\n",
      "Epoch 939/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2322 - accuracy: 0.9372 - val_loss: 0.3539 - val_accuracy: 0.9078\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.29067\n",
      "Epoch 940/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1904 - accuracy: 0.9453 - val_loss: 0.3379 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.29067\n",
      "Epoch 941/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2237 - accuracy: 0.9349 - val_loss: 0.3748 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.29067\n",
      "Epoch 942/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2407 - accuracy: 0.9366 - val_loss: 0.3483 - val_accuracy: 0.9084\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.29067\n",
      "Epoch 943/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2423 - accuracy: 0.9321 - val_loss: 0.3513 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.29067\n",
      "Epoch 944/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2258 - accuracy: 0.9369 - val_loss: 0.3184 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.29067\n",
      "Epoch 945/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2385 - accuracy: 0.9329 - val_loss: 0.3425 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.29067\n",
      "Epoch 946/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2005 - accuracy: 0.9410 - val_loss: 0.3427 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.29067\n",
      "Epoch 947/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2046 - accuracy: 0.9426 - val_loss: 0.3380 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.29067\n",
      "Epoch 948/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2082 - accuracy: 0.9384 - val_loss: 0.3429 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.29067\n",
      "Epoch 949/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2169 - accuracy: 0.9386 - val_loss: 0.3334 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.29067\n",
      "Epoch 950/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2138 - accuracy: 0.9379 - val_loss: 0.3412 - val_accuracy: 0.9107\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.29067\n",
      "Epoch 951/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2111 - accuracy: 0.9389 - val_loss: 0.3668 - val_accuracy: 0.9056\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.29067\n",
      "Epoch 952/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1932 - accuracy: 0.9417 - val_loss: 0.3489 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.29067\n",
      "Epoch 953/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2009 - accuracy: 0.9430 - val_loss: 0.3544 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.29067\n",
      "Epoch 954/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2102 - accuracy: 0.9407 - val_loss: 0.3822 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.29067\n",
      "Epoch 955/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2019 - accuracy: 0.9445 - val_loss: 0.3547 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.29067\n",
      "Epoch 956/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2212 - accuracy: 0.9397 - val_loss: 0.3548 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.29067\n",
      "Epoch 957/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2051 - accuracy: 0.9424 - val_loss: 0.3442 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.29067\n",
      "Epoch 958/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2140 - accuracy: 0.9377 - val_loss: 0.3611 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.29067\n",
      "Epoch 959/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2016 - accuracy: 0.9414 - val_loss: 0.3578 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.29067\n",
      "Epoch 960/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2031 - accuracy: 0.9420 - val_loss: 0.3653 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.29067\n",
      "Epoch 961/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2021 - accuracy: 0.9397 - val_loss: 0.3471 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.29067\n",
      "Epoch 962/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2119 - accuracy: 0.9429 - val_loss: 0.3477 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.29067\n",
      "Epoch 963/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2309 - accuracy: 0.9349 - val_loss: 0.3638 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.29067\n",
      "Epoch 964/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2160 - accuracy: 0.9369 - val_loss: 0.3790 - val_accuracy: 0.9147\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.29067\n",
      "Epoch 965/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2150 - accuracy: 0.9374 - val_loss: 0.3579 - val_accuracy: 0.9124\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.29067\n",
      "Epoch 966/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2338 - accuracy: 0.9383 - val_loss: 0.3536 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.29067\n",
      "Epoch 967/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2150 - accuracy: 0.9356 - val_loss: 0.3648 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.29067\n",
      "Epoch 968/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2149 - accuracy: 0.9350 - val_loss: 0.3381 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.29067\n",
      "Epoch 969/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2155 - accuracy: 0.9390 - val_loss: 0.3586 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.29067\n",
      "Epoch 970/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1972 - accuracy: 0.9409 - val_loss: 0.3680 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.29067\n",
      "Epoch 971/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1894 - accuracy: 0.9403 - val_loss: 0.3851 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.29067\n",
      "Epoch 972/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2154 - accuracy: 0.9392 - val_loss: 0.3654 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.29067\n",
      "Epoch 973/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2157 - accuracy: 0.9366 - val_loss: 0.3921 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.29067\n",
      "Epoch 974/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2328 - accuracy: 0.9357 - val_loss: 0.3809 - val_accuracy: 0.9090\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.29067\n",
      "Epoch 975/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1952 - accuracy: 0.9430 - val_loss: 0.3543 - val_accuracy: 0.9159\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.29067\n",
      "Epoch 976/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1957 - accuracy: 0.9450 - val_loss: 0.3723 - val_accuracy: 0.9118\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.29067\n",
      "Epoch 977/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1957 - accuracy: 0.9423 - val_loss: 0.4133 - val_accuracy: 0.9101\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.29067\n",
      "Epoch 978/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.1966 - accuracy: 0.9382 - val_loss: 0.3723 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.29067\n",
      "Epoch 979/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2111 - accuracy: 0.9373 - val_loss: 0.3532 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.29067\n",
      "Epoch 980/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2214 - accuracy: 0.9403 - val_loss: 0.3538 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.29067\n",
      "Epoch 981/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1961 - accuracy: 0.9433 - val_loss: 0.3526 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.29067\n",
      "Epoch 982/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2209 - accuracy: 0.9424 - val_loss: 0.3539 - val_accuracy: 0.9153\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.29067\n",
      "Epoch 983/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9406 - val_loss: 0.3683 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.29067\n",
      "Epoch 984/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2124 - accuracy: 0.9430 - val_loss: 0.3575 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.29067\n",
      "Epoch 985/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2003 - accuracy: 0.9419 - val_loss: 0.3643 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.29067\n",
      "Epoch 986/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2212 - accuracy: 0.9367 - val_loss: 0.3730 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.29067\n",
      "Epoch 987/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2002 - accuracy: 0.9403 - val_loss: 0.3491 - val_accuracy: 0.9164\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.29067\n",
      "Epoch 988/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1929 - accuracy: 0.9450 - val_loss: 0.3612 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.29067\n",
      "Epoch 989/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2014 - accuracy: 0.9412 - val_loss: 0.3345 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.29067\n",
      "Epoch 990/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2129 - accuracy: 0.9380 - val_loss: 0.3615 - val_accuracy: 0.9170\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.29067\n",
      "Epoch 991/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2144 - accuracy: 0.9409 - val_loss: 0.3179 - val_accuracy: 0.9227\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.29067\n",
      "Epoch 992/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2090 - accuracy: 0.9390 - val_loss: 0.3633 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.29067\n",
      "Epoch 993/1000\n",
      "140/140 [==============================] - 1s 4ms/step - loss: 0.2188 - accuracy: 0.9396 - val_loss: 0.3439 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.29067\n",
      "Epoch 994/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.1965 - accuracy: 0.9427 - val_loss: 0.3693 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.29067\n",
      "Epoch 995/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2199 - accuracy: 0.9372 - val_loss: 0.3626 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.29067\n",
      "Epoch 996/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2189 - accuracy: 0.9390 - val_loss: 0.3379 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.29067\n",
      "Epoch 997/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2303 - accuracy: 0.9347 - val_loss: 0.3587 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.29067\n",
      "Epoch 998/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2171 - accuracy: 0.9369 - val_loss: 0.3886 - val_accuracy: 0.9113\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.29067\n",
      "Epoch 999/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2225 - accuracy: 0.9347 - val_loss: 0.3931 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.29067\n",
      "Epoch 1000/1000\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.2107 - accuracy: 0.9423 - val_loss: 0.3820 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.29067\n",
      "Training completed in time:  0:12:34.839089\n"
     ]
    }
   ],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 1000\n",
    "num_batch_size = 50\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "virgin-butter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9141384959220886\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "chubby-newsletter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3710762e+02  8.3305443e+01 -6.8262817e+01  3.2430061e+01\n",
      " -1.5760985e+01  5.0360260e+00 -1.4178017e+01  6.8758984e+00\n",
      "  1.5025976e+00 -1.1992622e+01 -2.8097615e+00  2.2380594e+01\n",
      " -9.4585447e+00 -2.3872155e-01 -4.4718695e+00  3.6893067e+00\n",
      " -4.4111514e+00  2.4554980e+00 -5.6874247e+00 -3.7479608e+00\n",
      " -4.1549158e+00  1.0180766e+01  3.2225592e+00  2.0709116e+00\n",
      " -3.7253852e+00 -1.5346245e-01  4.5417762e+00  1.8417475e+00\n",
      " -6.8128147e+00  4.4453984e-01  3.5543990e+00  1.8905276e+00\n",
      "  6.3908499e-01 -2.4118891e+00  6.8140310e-01 -3.8383472e+00\n",
      "  1.0076929e+00  4.0431581e+00 -2.8989244e+00  1.7436763e+00]\n",
      "[[-2.3710762e+02  8.3305443e+01 -6.8262817e+01  3.2430061e+01\n",
      "  -1.5760985e+01  5.0360260e+00 -1.4178017e+01  6.8758984e+00\n",
      "   1.5025976e+00 -1.1992622e+01 -2.8097615e+00  2.2380594e+01\n",
      "  -9.4585447e+00 -2.3872155e-01 -4.4718695e+00  3.6893067e+00\n",
      "  -4.4111514e+00  2.4554980e+00 -5.6874247e+00 -3.7479608e+00\n",
      "  -4.1549158e+00  1.0180766e+01  3.2225592e+00  2.0709116e+00\n",
      "  -3.7253852e+00 -1.5346245e-01  4.5417762e+00  1.8417475e+00\n",
      "  -6.8128147e+00  4.4453984e-01  3.5543990e+00  1.8905276e+00\n",
      "   6.3908499e-01 -2.4118891e+00  6.8140310e-01 -3.8383472e+00\n",
      "   1.0076929e+00  4.0431581e+00 -2.8989244e+00  1.7436763e+00]]\n",
      "(1, 40)\n",
      "[2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['children_playing'], dtype='<U16')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"tests/test_childrenplay0.wav\"\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "prediction_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
